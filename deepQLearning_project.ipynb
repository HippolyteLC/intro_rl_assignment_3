{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0c69620",
   "metadata": {},
   "source": [
    "**Deep Q Learning Project RL**\n",
    "\n",
    "General Task Description:\n",
    "\n",
    "-> Given a fixed neural network, you will train an agent which will be able to play cartpole V1 on different pole lengths, e.g. make a generalist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987ef14c",
   "metadata": {},
   "source": [
    "1. IMPORT WHAT WE NEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e427690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from test_script import QNetwork\n",
    "from test_script import bar_plot, test_pole_length, test_script\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f732c7",
   "metadata": {},
   "source": [
    "2. THE BASE DEEP Q LEARNING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da7f516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replay buffer\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Replay Buffer to store experience tuples for deep q learning.\n",
    "    The replay buffer stores experiences from many episodes and randomly samples them during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "def select_action(state, policy_net, epsilon, action_dim):\n",
    "    \"\"\"\n",
    "    Select action using epsilon-greedy policy - did it with epsilon-greedy because of Assignent 1\n",
    "    \"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(action_dim)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_values = policy_net(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "\n",
    "def deep_q_learning(epsilon, gamma, alpha, q_network, n_episodes, \n",
    "                    pole_lengths=None, env_name='CartPole-v1',\n",
    "                    batch_size=64, buffer_capacity=50000, \n",
    "                    update_target_every=10, epsilon_decay=0.995, \n",
    "                    epsilon_min=0.01):\n",
    "    \"\"\"\n",
    "    Deep q learning agent for CartPole-v1 environment with varying pole lengths.\n",
    "    \n",
    "    param: epsilon : float - initial exploration rate\n",
    "    param: gamma : float - discount factor\n",
    "    param: alpha : float - learning rate\n",
    "    param: q_network : QNetwork or None - pre-initialized network or None to create new one\n",
    "    param: n_episodes : int - number of training episodes\n",
    "    param: pole_lengths : array-like or None - array of pole lengths to train on (default: linspace(0.4, 1.8, 30))\n",
    "    param: env_name : str - gym environment name\n",
    "    param: batch_size : int - batch size for training\n",
    "    param: buffer_capacity : int - replay buffer capacity\n",
    "    param: update_target_every : int - how often to update target network\n",
    "    param: epsilon_decay : float - epsilon decay rate per episode\n",
    "    param: epsilon_min : float - minimum epsilon value\n",
    "\n",
    "    return: tuple : (policy_net, target_net, episode_returns)\n",
    "        - trained networks and list of episode rewards\n",
    "    \"\"\"\n",
    "\n",
    "    # initialization of environment\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    # initialization of networks if not provided\n",
    "    if q_network is None:\n",
    "        policy_net = QNetwork(state_dim, action_dim)\n",
    "        target_net = QNetwork(state_dim, action_dim)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        target_net.eval()\n",
    "    else:\n",
    "        policy_net = q_network\n",
    "        target_net = QNetwork(state_dim, action_dim)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        target_net.eval()\n",
    "\n",
    "    # initialization of optimizer\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=alpha)\n",
    "\n",
    "    # initialization of replay buffer\n",
    "    replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "    \n",
    "    # pole lengths for training\n",
    "    if pole_lengths is None:\n",
    "        pole_lengths = np.linspace(0.4, 1.8, 30)\n",
    "\n",
    "    # storing episode returns for plotting\n",
    "    episode_returns = []\n",
    "    \n",
    "    # copy of current epsilon value for decay\n",
    "    epsi = epsilon\n",
    "    \n",
    "    # training loop\n",
    "    for episode in range(n_episodes):\n",
    "        # randomly select pole length for this episode (we need to figure an experimental setup)\n",
    "        pole_length = np.random.choice(pole_lengths)\n",
    "        env.unwrapped.length = pole_length\n",
    "        \n",
    "        # reset environment\n",
    "        state = env.reset()[0]\n",
    "        episode_reward = 0.0\n",
    "        \n",
    "        # epsilon decay\n",
    "        if epsi > epsilon_min:\n",
    "            epsi = max(epsilon_min, epsi * epsilon_decay)\n",
    "        \n",
    "        # episode loop (1 episode = 1 pole length)\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # select action\n",
    "            action = select_action(state, policy_net, epsi, action_dim)\n",
    "            \n",
    "            # take step\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # store transition in replay buffer\n",
    "            replay_buffer.push(state, action, reward, next_state, float(done))\n",
    "\n",
    "            # deep q learning update (using mini-batch from replay buffer)\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                # sample batch from replay buffer\n",
    "                batch = replay_buffer.sample(batch_size)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "                \n",
    "                # convert to tensors \n",
    "                states_t = torch.FloatTensor(states)\n",
    "                actions_t = torch.LongTensor(actions).unsqueeze(1)\n",
    "                rewards_t = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "                next_states_t = torch.FloatTensor(next_states)\n",
    "                dones_t = torch.FloatTensor(dones).unsqueeze(1)\n",
    "\n",
    "                #get current q values\n",
    "                current_q = policy_net(states_t).gather(1, actions_t)\n",
    "                \n",
    "                # target values\n",
    "                with torch.no_grad():\n",
    "                    next_max = target_net(next_states_t).max(1)[0].unsqueeze(1)\n",
    "                    td_target = rewards_t + gamma * next_max * (1 - dones_t)\n",
    "                \n",
    "                # loss calc\n",
    "                loss = nn.MSELoss()(current_q, td_target)\n",
    "                \n",
    "                # backprop and optimize + gradient clipping\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        \n",
    "        \n",
    "        #update target network periodically\n",
    "        if episode % update_target_every == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        #store episode reward\n",
    "        episode_returns.append(episode_reward)\n",
    "        \n",
    "        #only for seeing the progress\n",
    "        if episode % 100 == 0:\n",
    "            avg_reward = np.mean(episode_returns[-100:]) if len(episode_returns) >= 100 else np.mean(episode_returns)\n",
    "            print(f\"Episode {episode}/{n_episodes} | \"\n",
    "                  f\"Avg Reward: {avg_reward:.1f} | \"\n",
    "                  f\"Epsilon: {epsi:.3f}\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    return (policy_net, target_net, episode_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38c3c1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ([0, 0, 0, 0], 1, 1.0, [1, 1, 1, 1], False)\n"
     ]
    }
   ],
   "source": [
    "#Test Replay Buffer\n",
    "# --- IGNORE ---\n",
    "rb=ReplayBuffer(3)\n",
    "rb.push([0,0,0,0], 1, 1.0, [1,1,1,1], False)\n",
    "print(len(rb), rb.sample(1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b7d8f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/400 | Avg Reward: 12.0 | Epsilon: 0.980\n",
      "Episode 100/400 | Avg Reward: 126.2 | Epsilon: 0.130\n",
      "Episode 200/400 | Avg Reward: 272.3 | Epsilon: 0.050\n",
      "Episode 300/400 | Avg Reward: 252.0 | Epsilon: 0.050\n",
      "----finished training----\n",
      "Last 3 episode returns: [500.0, 214.0, 500.0]\n"
     ]
    }
   ],
   "source": [
    "#Test the deep q learning function\n",
    "# --- IGNORE ---\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# --- training test ---\n",
    "policy_net, target_net, returns = deep_q_learning(\n",
    "    epsilon=1.0,\n",
    "    gamma=0.99,\n",
    "    alpha=1e-3,\n",
    "    q_network=None,           # create fresh nets\n",
    "    n_episodes=400, \n",
    "    pole_lengths=np.linspace(0.4, 1.2, 5),\n",
    "    env_name='CartPole-v1',\n",
    "    batch_size=32,\n",
    "    buffer_capacity=10000,\n",
    "    update_target_every=5,\n",
    "    epsilon_decay=0.98,\n",
    "    epsilon_min=0.05\n",
    ")\n",
    "\n",
    "print(\"----finished training----\")\n",
    "print(\"Last 3 episode returns:\", returns[-3:] if len(returns) >= 3 else returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b4c126",
   "metadata": {},
   "source": [
    "2.1 Approach 1: Adaptive Reward \n",
    "\n",
    "Smth like: reward = reward + abs((1 - abs(angle)/12 degrees)) + abs((1 - abs(position)/2.4)) - 0.2*abs(angular velocity) - 0.2*abs(cartÂ velocity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a90838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1374d98",
   "metadata": {},
   "source": [
    "2.2 Approach 2: Scrappy Adversial - hitting the weak spots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c7799a",
   "metadata": {},
   "source": [
    "2.2.1 Adaptive curriculum learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6d036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveCurriculumLearning:\n",
    "    \"\"\" \n",
    "    Adaptive curriculum learning offers an object to store the accumulated rewards, \n",
    "    performances, difficulty socores, and probabilities for each pole length. \n",
    "    We check the last n (LOOK_BACK_WINDOW) rewards a pole has obtained and assign probabilities \n",
    "    to sample each length for an episode. The goal is to attack the policy on its weak spots, \n",
    "    we prioritize training on weak performing lengths. This should shift performance metrics and keeps \n",
    "    our attacks adaptive as we target weakest. \n",
    "    param: all_pole_lengths : numpy.ndarray - stores all the pole lengths\n",
    "    \"\"\"\n",
    "    LOOK_BACK_WINDOW = 20 # we only consider the last 20 rewards a pole has gotten (captures more sensitive information)\n",
    "    P_ADAPTIVE = 0.8 # probability of using adaptive probability distribution or uniform random\n",
    "    def __init__(self, all_pole_lengths):\n",
    "        self.all_pole_lengths = all_pole_lengths\n",
    "        self.rewards = defaultdict(list) # storing pole length episode reward for approach 2 (see 2.2)\n",
    "        self.performances = {} # keep track of performance metric for each pole length \n",
    "        self.difficulty_scores = {} # difficulty_scores\n",
    "        self.distribution = {} # probability distribution for pole lengths to sample from\n",
    "        \n",
    "        # initialize difficulties and probabilities (initially prob is uniform)\n",
    "        initial_prob = 1.0 / len(all_pole_lengths)\n",
    "        self.i_p = initial_prob\n",
    "        for length in all_pole_lengths:\n",
    "            self.difficulty_scores[length] = 1.0\n",
    "            self.distribution[length] = initial_prob\n",
    "\n",
    "    def update_rewards(self, pole_length, reward):\n",
    "        self.rewards[pole_length].append(reward)\n",
    "\n",
    "    def update_performances(self, pole_length):\n",
    "        \"\"\"\n",
    "        Here we update the performance of a pole, \n",
    "        \"\"\"\n",
    "        reward_list = self.rewards[pole_length]\n",
    "\n",
    "        # if a pole has not been played yet, it will be assigned a performance metric of 0\n",
    "        if not reward_list:\n",
    "            metric = 0\n",
    "        else:\n",
    "            # here we use a LOOK BACK WINDOW so that we dont use over-stabilized episode rewards\n",
    "            metric = np.mean(reward_list[-self.LOOK_BACK_WINDOW:])\n",
    "        self.performances[pole_length] = metric\n",
    "\n",
    "    def update_difficulties(self):\n",
    "        \"\"\"\n",
    "        Difficulties are inversely proportional to the performance metrics.\n",
    "        The worse a pole length has performed, the higher the diff score (diff=1 being the worst performing, diff=0 being best).\n",
    "        These require global updates as the update is relative to the totality of pole lengths.\n",
    "        Also normalizing + scaling the values down. Metrics can offer quite larger values otherwise. \n",
    "        \"\"\"\n",
    "        M_max = self.find_max()\n",
    "        M_min = self.find_min()s\n",
    "        diff_M = M_max - M_min\n",
    "\n",
    "        # update all difficulty scores with new information\n",
    "        for pole_length, metric in self.performances.items():\n",
    "            if diff_M == 0:\n",
    "                difficulty = 1\n",
    "            else:\n",
    "                difficulty = 1 - ((metric-M_min) / diff_M) # if best perform, diff is 0 >>> probability assignment will be 0\n",
    "            self.difficulty_scores[pole_length] = difficulty\n",
    "    \n",
    "    def update_distribution(self):\n",
    "        \"\"\" \n",
    "        Distribution is also global, here we require an update proportional to the totality. \n",
    "        Normalizing the probabilities (between 0 and 1)\n",
    "        \"\"\"\n",
    "        if not self.difficulty_scores:\n",
    "            return\n",
    "        total_difficulty = sum(self.difficulty_scores.values())\n",
    "\n",
    "        if total_difficulty > 0:\n",
    "            for pole_length, difficulty in self.difficulty_scores.items():\n",
    "                self.distribution[pole_length] = difficulty / total_difficulty\n",
    "        else:\n",
    "            for pole_length in self.all_pole_lengths:\n",
    "                self.distribution[pole_length] = self.i_p\n",
    "    \n",
    "    def sample_length(self):\n",
    "        \"\"\"\n",
    "        Here select the pole length using either uniform or categorical prob distribution. \n",
    "        \"\"\"\n",
    "        if random.random() < self.P_ADAPTIVE:\n",
    "            pole_lengths = list(self.distribution.keys())\n",
    "            probs = list(self.distribution.values())\n",
    "\n",
    "            # if our prob dist is empty, we fallback to uniform selection\n",
    "            if not pole_lengths or sum(probs) == 0:\n",
    "                return np.random.choice(self.all_pole_lengths)\n",
    "            \n",
    "            # selection based on categorical sampling (discrete prob distribution)\n",
    "            return np.random.choice(a=pole_lengths, p=probs, size=1)[0]\n",
    "        else:\n",
    "            return np.random.choice(self.all_pole_lengths)\n",
    "\n",
    "    def find_max(self):\n",
    "        \"\"\"\n",
    "        Find max performing pole length\n",
    "        \"\"\"\n",
    "        if not self.performances:\n",
    "            return 0\n",
    "        return max(self.performances.values())\n",
    "\n",
    "    def find_min(self):\n",
    "        \"\"\"\n",
    "        Find min performing pole length\n",
    "        \"\"\"\n",
    "        if not self.performances:\n",
    "            return 0\n",
    "        return min(self.performances.values())\n",
    "    \n",
    "    def calculate_pole_stats(self):\n",
    "        \"\"\"\n",
    "        Nice display function for avg reward of each pole. Not functionally important at all. \n",
    "        \"\"\"\n",
    "        avg_pole_stats = {}\n",
    "        \n",
    "        for pole_length, rewards_list in self.rewards.items():\n",
    "            # Only process if the list is not empty\n",
    "            if rewards_list:\n",
    "                avg_reward = np.mean(rewards_list)\n",
    "                count = len(rewards_list)\n",
    "            else:\n",
    "                avg_reward = 0.0\n",
    "                count = 0\n",
    "            avg_pole_stats[pole_length] = {\n",
    "                \"average_reward\": avg_reward,\n",
    "                \"episode_count\": count\n",
    "            }\n",
    "        for p, avg in avg_pole_stats.items():\n",
    "            print(f\"Pole length {p} has avg reward {avg}\")\n",
    "        return avg_pole_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0bbd394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Replay Buffer to store experience tuples for deep q learning.\n",
    "    The replay buffer stores experiences from many episodes and randomly samples them during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "def select_action(state, policy_net, epsilon, action_dim):\n",
    "    \"\"\"\n",
    "    Select action using epsilon-greedy policy - did it with epsilon-greedy because of Assignent 1\n",
    "    \"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(action_dim)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_values = policy_net(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "\n",
    "def deep_q_learning(epsilon, gamma, alpha, q_network, n_episodes,\n",
    "                    uniform_episode_training_cap=300,\n",
    "                    pole_lengths=None, env_name='CartPole-v1',\n",
    "                    batch_size=64, buffer_capacity=50000, \n",
    "                    update_target_every=10, epsilon_decay=0.995, \n",
    "                    epsilon_min=0.01):\n",
    "    \"\"\"\n",
    "    Deep q learning agent for CartPole-v1 environment with varying pole lengths.\n",
    "    \n",
    "    param: epsilon : float - initial exploration rate\n",
    "    param: gamma : float - discount factor\n",
    "    param: alpha : float - learning rate\n",
    "    param: q_network : QNetwork or None - pre-initialized network or None to create new one\n",
    "    param: n_episodes : int - number of training episodes\n",
    "    param: uniform_episode_training_cap : int - number episodes trained with uniform length selection\n",
    "    param: p_adaptive : float - probability to select pole length from sample distribution, after uniform_episode_training_cap\n",
    "    param: lb_window : int - look back window used to compute performance metric, number of recent pole length performances\n",
    "    param: pole_lengths : array-like or None - array of pole lengths to train on (default: linspace(0.4, 1.8, 30))\n",
    "    param: env_name : str - gym environment name\n",
    "    param: batch_size : int - batch size for training\n",
    "    param: buffer_capacity : int - replay buffer capacity\n",
    "    param: update_target_every : int - how often to update target network\n",
    "    param: epsilon_decay : float - epsilon decay rate per episode\n",
    "    param: epsilon_min : float - minimum epsilon value\n",
    "\n",
    "    return: tuple : (policy_net, target_net, episode_returns)\n",
    "        - trained networks and list of episode rewards\n",
    "    \"\"\"\n",
    "\n",
    "    # initialization of environment\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    # initialization of networks if not provided\n",
    "    if q_network is None:\n",
    "        policy_net = QNetwork(state_dim, action_dim)\n",
    "        target_net = QNetwork(state_dim, action_dim)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        target_net.eval()\n",
    "    else:\n",
    "        policy_net = q_network\n",
    "        target_net = QNetwork(state_dim, action_dim)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        target_net.eval()\n",
    "\n",
    "    # initialization of optimizer\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=alpha)\n",
    "\n",
    "    # initialization of replay buffer\n",
    "    replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "    \n",
    "    # pole lengths for training\n",
    "    if pole_lengths is None:\n",
    "        pole_lengths = np.linspace(0.4, 1.8, 30)\n",
    "\n",
    "    # storing episode returns for plotting\n",
    "    episode_returns = []\n",
    "\n",
    "    # initialize the acl class for adaptive hyperparametre sampling\n",
    "    acl = AdaptiveCurriculumLearning(pole_lengths)\n",
    "\n",
    "    # copy of current epsilon value for decay\n",
    "    epsi = epsilon\n",
    "    \n",
    "    # training loop\n",
    "    for episode in range(n_episodes):\n",
    "        # if current episode is below the uniform_episode_training_cap we select from a uniform distr\n",
    "        if episode < uniform_episode_training_cap:\n",
    "            pole_length = np.random.choice(pole_lengths)\n",
    "        # else use adaptive probability distrubition \n",
    "        else: \n",
    "            pole_length = acl.sample_length()\n",
    "\n",
    "        env.unwrapped.length = pole_length\n",
    "        \n",
    "        # reset environment\n",
    "        state = env.reset()[0]\n",
    "        episode_reward = 0.0\n",
    "        \n",
    "        # epsilon decay\n",
    "        if epsi > epsilon_min:\n",
    "            epsi = max(epsilon_min, epsi * epsilon_decay)\n",
    "        \n",
    "        # episode loop (1 episode = 1 pole length)\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # select action\n",
    "            action = select_action(state, policy_net, epsi, action_dim)\n",
    "            \n",
    "            # take step\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # store transition in replay buffer\n",
    "            replay_buffer.push(state, action, reward, next_state, float(done))\n",
    "\n",
    "            # deep q learning update (using mini-batch from replay buffer)\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                # sample batch from replay buffer\n",
    "                batch = replay_buffer.sample(batch_size)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "                \n",
    "                # convert to tensors \n",
    "                states_t = torch.FloatTensor(states)\n",
    "                actions_t = torch.LongTensor(actions).unsqueeze(1)\n",
    "                rewards_t = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "                next_states_t = torch.FloatTensor(next_states)\n",
    "                dones_t = torch.FloatTensor(dones).unsqueeze(1)\n",
    "\n",
    "                #get current q values\n",
    "                current_q = policy_net(states_t).gather(1, actions_t)\n",
    "                \n",
    "                # target values\n",
    "                with torch.no_grad():\n",
    "                    next_max = target_net(next_states_t).max(1)[0].unsqueeze(1)\n",
    "                    td_target = rewards_t + gamma * next_max * (1 - dones_t)\n",
    "                \n",
    "                # loss calc\n",
    "                loss = nn.MSELoss()(current_q, td_target)\n",
    "                \n",
    "                # backprop and optimize + gradient clipping\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        \n",
    "        \n",
    "        #update target network periodically\n",
    "        if episode % update_target_every == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        #store episode reward\n",
    "        episode_returns.append(episode_reward)\n",
    "\n",
    "        # update acl for rewards, performance metrics, scores, and probs\n",
    "        # difficulty scores and probabilities are global updates, can be seen in the respective update methods\n",
    "        acl.update_rewards(pole_length, episode_reward)\n",
    "        if episode >= uniform_episode_training_cap:\n",
    "            acl.update_performances(pole_length)\n",
    "            acl.update_difficulties()\n",
    "            acl.update_distribution()\n",
    "\n",
    "        #only for seeing the progress\n",
    "        if episode % 50 == 0:\n",
    "            avg_reward = np.mean(episode_returns[-100:]) if len(episode_returns) >= 100 else np.mean(episode_returns)\n",
    "            print(f\"Episode {episode}/{n_episodes} | \"\n",
    "                  f\"Avg Reward: {avg_reward:.1f} | \"\n",
    "                  f\"Epsilon: {epsi:.3f} | \"\n",
    "                  f\"Probability Distribution{acl.distribution} | \\n\" \n",
    "                  f\"Pole Length Performances{acl.performances} | \\n\"\n",
    "                  )\n",
    "            \n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    return policy_net, target_net, episode_returns, acl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa06100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/200 | Avg Reward: 12.0 | Epsilon: 0.980 | {np.float64(0.4): 0.2, np.float64(0.6): 0.2, np.float64(0.8): 0.2, np.float64(1.0): 0.2, np.float64(1.2): 0.2} | \n",
      "{} | \n",
      "\n",
      "Episode 50/200 | Avg Reward: 59.7 | Epsilon: 0.357 | {np.float64(0.4): 0.2, np.float64(0.6): 0.2, np.float64(0.8): 0.2, np.float64(1.0): 0.2, np.float64(1.2): 0.2} | \n",
      "{} | \n",
      "\n",
      "Episode 100/200 | Avg Reward: 107.6 | Epsilon: 0.130 | {np.float64(0.4): 0.2, np.float64(0.6): 0.2, np.float64(0.8): 0.2, np.float64(1.0): 0.2, np.float64(1.2): 0.2} | \n",
      "{np.float64(1.0): np.float64(76.96296296296296)} | \n",
      "\n",
      "Episode 150/200 | Avg Reward: 152.8 | Epsilon: 0.050 | {np.float64(0.4): np.float64(0.0757035944309186), np.float64(0.6): np.float64(0.0), np.float64(0.8): np.float64(0.3440014099438963), np.float64(1.0): np.float64(0.4083047284174575), np.float64(1.2): np.float64(0.17199026720772756)} | \n",
      "{np.float64(1.0): np.float64(105.02272727272727), np.float64(1.2): np.float64(127.34482758620689), np.float64(0.8): np.float64(111.09677419354838), np.float64(0.6): np.float64(143.5909090909091), np.float64(0.4): np.float64(136.44)} | \n",
      "\n",
      "----finished training----\n",
      "Last 3 episode returns: [137.0, 111.0, 121.0]\n"
     ]
    }
   ],
   "source": [
    "#Test the deep q learning function\n",
    "# --- IGNORE ---\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# --- training test ---\n",
    "policy_net, target_net, returns, acl = deep_q_learning(\n",
    "    epsilon=1.0,\n",
    "    gamma=0.99,\n",
    "    alpha=1e-3,\n",
    "    q_network=None,           # create fresh nets\n",
    "    n_episodes=200, \n",
    "    uniform_episode_training_cap=100, # here you set the moment from which adaptive CL is applied \n",
    "    pole_lengths=np.linspace(0.4, 1.2, 5),\n",
    "    env_name='CartPole-v1',\n",
    "    batch_size=32,\n",
    "    buffer_capacity=10000,\n",
    "    update_target_every=5,\n",
    "    epsilon_decay=0.98,\n",
    "    epsilon_min=0.05\n",
    ")\n",
    "\n",
    "print(\"----finished training----\")\n",
    "print(\"Last 3 episode returns:\", returns[-3:] if len(returns) >= 3 else returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ef8394d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "{np.float64(1.0): np.float64(106.85483870967742), np.float64(1.2): np.float64(127.03030303030303), np.float64(0.8): np.float64(113.90196078431373), np.float64(0.6): np.float64(141.52173913043478), np.float64(0.4): np.float64(133.3548387096774)}\n",
      "{np.float64(0.4): np.float64(0.2355820774754731), np.float64(0.6): np.float64(0.0), np.float64(0.8): np.float64(0.7967190031671615), np.float64(1.0): np.float64(1.0), np.float64(1.2): np.float64(0.4180193765305529)}\n",
      "{np.float64(0.4): np.float64(0.09614337454752851), np.float64(0.6): np.float64(0.0), np.float64(0.8): np.float64(0.32514890076308484), np.float64(1.0): np.float64(0.4081098850040415), np.float64(1.2): np.float64(0.17059783968534506)}\n"
     ]
    }
   ],
   "source": [
    "# acl.calculate_pole_stats()\n",
    "print(type(acl.all_pole_lengths))\n",
    "print(acl.performances)\n",
    "print(acl.difficulty_scores)\n",
    "print(acl.distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db0ea50",
   "metadata": {},
   "source": [
    "2.3 Approach 3: ??SOMETHING WITH REPLAY BUFFER??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb112434",
   "metadata": {},
   "source": [
    "3. MAIN TRAINING FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437483ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88137ab5",
   "metadata": {},
   "source": [
    "4. TESTING BUT JUST FOR US - not using test_script.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7451c580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47c7bc6e",
   "metadata": {},
   "source": [
    "5. MAIN EXECUTION WITH PLOTTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25e55ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "introrl25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
