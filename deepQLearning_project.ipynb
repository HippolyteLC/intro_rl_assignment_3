{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0c69620",
   "metadata": {},
   "source": [
    "**Deep Q Learning Project RL**\n",
    "\n",
    "General Task Description:\n",
    "\n",
    "-> Given a fixed neural network, you will train an agent which will be able to play cartpole V1 on different pole lengths, e.g. make a generalist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987ef14c",
   "metadata": {},
   "source": [
    "1. IMPORT WHAT WE NEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e427690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from test_script import QNetwork\n",
    "from test_script import bar_plot, test_pole_length, test_script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f732c7",
   "metadata": {},
   "source": [
    "2. THE BASE DEEP Q LEARNING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7da7f516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replay buffer\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Replay Buffer to store experience tuples for deep q learning.\n",
    "    The replay buffer stores experiences from many episodes and randomly samples them during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "def select_action(state, policy_net, epsilon, action_dim):\n",
    "    \"\"\"\n",
    "    Select action using epsilon-greedy policy - did it with epsilon-greedy because of Assignent 1\n",
    "    \"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(action_dim)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_values = policy_net(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "\n",
    "def deep_q_learning(epsilon, gamma, alpha, q_network, n_episodes, \n",
    "                    pole_lengths=None, env_name='CartPole-v1',\n",
    "                    batch_size=64, buffer_capacity=50000, \n",
    "                    update_target_every=10, epsilon_decay=0.995, \n",
    "                    epsilon_min=0.01):\n",
    "    \"\"\"\n",
    "    Deep q learning agent for CartPole-v1 environment with varying pole lengths.\n",
    "    \n",
    "    param: epsilon : float - initial exploration rate\n",
    "    param: gamma : float - discount factor\n",
    "    param: alpha : float - learning rate\n",
    "    param: q_network : QNetwork or None - pre-initialized network or None to create new one\n",
    "    param: n_episodes : int - number of training episodes\n",
    "    param: pole_lengths : array-like or None - array of pole lengths to train on (default: linspace(0.4, 1.8, 30))\n",
    "    param: env_name : str - gym environment name\n",
    "    param: batch_size : int - batch size for training\n",
    "    param: buffer_capacity : int - replay buffer capacity\n",
    "    param: update_target_every : int - how often to update target network\n",
    "    param: epsilon_decay : float - epsilon decay rate per episode\n",
    "    param: epsilon_min : float - minimum epsilon value\n",
    "\n",
    "    return: tuple : (policy_net, target_net, episode_returns)\n",
    "        - trained networks and list of episode rewards\n",
    "    \"\"\"\n",
    "\n",
    "    # initialization of environment\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    # initialization of networks if not provided\n",
    "    if q_network is None:\n",
    "        policy_net = QNetwork(state_dim, action_dim)\n",
    "        target_net = QNetwork(state_dim, action_dim)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        target_net.eval()\n",
    "    else:\n",
    "        policy_net = q_network\n",
    "        target_net = QNetwork(state_dim, action_dim)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        target_net.eval()\n",
    "\n",
    "    # initialization of optimizer\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=alpha)\n",
    "\n",
    "    # initialization of replay buffer\n",
    "    replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "    \n",
    "    # pole lengths for training\n",
    "    if pole_lengths is None:\n",
    "        pole_lengths = np.linspace(0.4, 1.8, 30)\n",
    "\n",
    "    # storing episode returns for plotting\n",
    "    episode_returns = []\n",
    "    \n",
    "    # copy of current epsilon value for decay\n",
    "    epsi = epsilon\n",
    "    \n",
    "    # training loop\n",
    "    for episode in range(n_episodes):\n",
    "        # randomly select pole length for this episode (we need to figure an experimental setup)\n",
    "        pole_length = np.random.choice(pole_lengths)\n",
    "        env.unwrapped.length = pole_length\n",
    "        \n",
    "        # reset environment\n",
    "        state = env.reset()[0]\n",
    "        episode_reward = 0.0\n",
    "        \n",
    "        # epsilon decay\n",
    "        if epsi > epsilon_min:\n",
    "            epsi = max(epsilon_min, epsi * epsilon_decay)\n",
    "        \n",
    "        # episode loop (1 episode = 1 pole length)\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # select action\n",
    "            action = select_action(state, policy_net, epsi, action_dim)\n",
    "            \n",
    "            # take step\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # store transition in replay buffer\n",
    "            replay_buffer.push(state, action, reward, next_state, float(done))\n",
    "\n",
    "            # deep q learning update (using mini-batch from replay buffer)\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                # sample batch from replay buffer\n",
    "                batch = replay_buffer.sample(batch_size)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "                \n",
    "                # convert to tensors \n",
    "                states_t = torch.FloatTensor(states)\n",
    "                actions_t = torch.LongTensor(actions).unsqueeze(1)\n",
    "                rewards_t = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "                next_states_t = torch.FloatTensor(next_states)\n",
    "                dones_t = torch.FloatTensor(dones).unsqueeze(1)\n",
    "\n",
    "                #get current q values\n",
    "                current_q = policy_net(states_t).gather(1, actions_t)\n",
    "                \n",
    "                # target values\n",
    "                with torch.no_grad():\n",
    "                    next_max = target_net(next_states_t).max(1)[0].unsqueeze(1)\n",
    "                    td_target = rewards_t + gamma * next_max * (1 - dones_t)\n",
    "                \n",
    "                # loss calc\n",
    "                loss = nn.MSELoss()(current_q, td_target)\n",
    "                \n",
    "                # backprop and optimize + gradient clipping\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        \n",
    "        \n",
    "        #update target network periodically\n",
    "        if episode % update_target_every == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        #store episode reward\n",
    "        episode_returns.append(episode_reward)\n",
    "        \n",
    "        #only for seeing the progress\n",
    "        if episode % 100 == 0:\n",
    "            avg_reward = np.mean(episode_returns[-100:]) if len(episode_returns) >= 100 else np.mean(episode_returns)\n",
    "            print(f\"Episode {episode}/{n_episodes} | \"\n",
    "                  f\"Avg Reward: {avg_reward:.1f} | \"\n",
    "                  f\"Epsilon: {epsi:.3f}\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    return (policy_net, target_net, episode_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38c3c1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ([0, 0, 0, 0], 1, 1.0, [1, 1, 1, 1], False)\n"
     ]
    }
   ],
   "source": [
    "#Test Replay Buffer\n",
    "# --- IGNORE ---\n",
    "rb=ReplayBuffer(3)\n",
    "rb.push([0,0,0,0], 1, 1.0, [1,1,1,1], False)\n",
    "print(len(rb), rb.sample(1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b7d8f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/400 | Avg Reward: 12.0 | Epsilon: 0.980\n",
      "Episode 100/400 | Avg Reward: 126.2 | Epsilon: 0.130\n",
      "Episode 200/400 | Avg Reward: 272.3 | Epsilon: 0.050\n",
      "Episode 300/400 | Avg Reward: 252.0 | Epsilon: 0.050\n",
      "----finished training----\n",
      "Last 3 episode returns: [500.0, 214.0, 500.0]\n"
     ]
    }
   ],
   "source": [
    "#Test the deep q learning function\n",
    "# --- IGNORE ---\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# --- training test ---\n",
    "policy_net, target_net, returns = deep_q_learning(\n",
    "    epsilon=1.0,\n",
    "    gamma=0.99,\n",
    "    alpha=1e-3,\n",
    "    q_network=None,           # create fresh nets\n",
    "    n_episodes=400, \n",
    "    pole_lengths=np.linspace(0.4, 1.2, 5),\n",
    "    env_name='CartPole-v1',\n",
    "    batch_size=32,\n",
    "    buffer_capacity=10000,\n",
    "    update_target_every=5,\n",
    "    epsilon_decay=0.98,\n",
    "    epsilon_min=0.05\n",
    ")\n",
    "\n",
    "print(\"----finished training----\")\n",
    "print(\"Last 3 episode returns:\", returns[-3:] if len(returns) >= 3 else returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b4c126",
   "metadata": {},
   "source": [
    "2.1 Approach 1: Adaptive Reward \n",
    "\n",
    "Smth like: reward = reward + abs((1 - abs(angle)/12 degrees)) + abs((1 - abs(position)/2.4)) - 0.2*abs(angular velocity) - 0.2*abs(cart velocity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a90838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1374d98",
   "metadata": {},
   "source": [
    "2.2 Approach 2: Scrappy Adversial - hitting the weak spots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbd394c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8db0ea50",
   "metadata": {},
   "source": [
    "2.3 Approach 3: ????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6276222",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb112434",
   "metadata": {},
   "source": [
    "3. MAIN TRAINING FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437483ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88137ab5",
   "metadata": {},
   "source": [
    "4. TESTING BUT JUST FOR US - not using test_script.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7451c580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47c7bc6e",
   "metadata": {},
   "source": [
    "5. MAIN EXECUTION WITH PLOTTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25e55ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
