{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02fef1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from test_script import QNetwork\n",
    "from test_script import bar_plot, test_pole_length, test_script\n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ae5dad",
   "metadata": {},
   "source": [
    "# Main Classes used in different strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e1aacf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replay buffer\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Replay Buffer to store experience tuples for deep q learning.\n",
    "    The replay buffer stores experiences from many episodes and randomly samples them during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.sampled_pole_lengths = []\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done, pole_length):\n",
    "        self.buffer.append((state, action, reward, next_state, done, pole_length))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95a82e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- MODIFIED REPLAY BUFFER FOR PER ---\n",
    "class PrioritizedReplayBuffer:\n",
    "    \"\"\"\n",
    "    Prioritized Replay Buffer using a simple list for storage and management.\n",
    "    NOTE: A proper, efficient PER implementation uses a SumTree/SegmentTree.\n",
    "    This simplified version is for conceptual demonstration but is O(N) for sampling.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity, alpha=0.6, epsilon=1e-5):\n",
    "        self.capacity = capacity\n",
    "        # Stored transitions\n",
    "        self.buffer = []\n",
    "        # Stored priorities (parallel list)\n",
    "        self.priorities = []\n",
    "        self.alpha = alpha  # PER exponent\n",
    "        self.epsilon = epsilon # Small constant to ensure non-zero probability\n",
    "        self.position = 0\n",
    "        self.sampled_pole_lengths = []\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done, pole_length):\n",
    "        max_priority = 1.0 if not self.priorities else max(self.priorities)\n",
    "        sample = (state, action, reward, next_state, done, pole_length)\n",
    "        \n",
    "        # If buffer is not full, append; otherwise, overwrite old sample\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(sample)\n",
    "            self.priorities.append(max_priority) # NEW: Assign max priority to new sample\n",
    "        else:\n",
    "            self.buffer[self.position] = sample\n",
    "            self.priorities[self.position] = max_priority # NEW: Assign max priority to overwritten sample\n",
    "        \n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        # Calculate P(i) based on priorities\n",
    "        # p_i = (priority + epsilon)^alpha\n",
    "        # P(i) = p_i / sum(p_k)\n",
    "        # here we are calculating the priorrities for the weights which will be used in training and sampling\n",
    "\n",
    "        scaled_priorities = np.array(self.priorities)**self.alpha\n",
    "        sum_priorities = scaled_priorities.sum()\n",
    "        prob_i = scaled_priorities / sum_priorities\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=prob_i)\n",
    "        \n",
    "        batch = [self.buffer[i] for i in indices]\n",
    "        \n",
    "\n",
    "        # here we calculate the sampling weights based on importance\n",
    "        N = len(self.buffer)\n",
    "        weights = (1.0 / N / prob_i[indices])**beta\n",
    "        \n",
    "        # Normalize weights by the maximum weight to stabilize training\n",
    "        max_weight = weights.max()\n",
    "        weights = weights / max_weight\n",
    "\n",
    "        return batch, indices, weights # NEW: Return indices and weights\n",
    "\n",
    "    def update_priorities(self, indices, errors): # NEW: Method to update priorities\n",
    "        for idx, error in zip(indices, errors):\n",
    "            # Prioritized Experience Replay uses the absolute TD error for priority\n",
    "            self.priorities[idx] = max(error, self.epsilon)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24c97e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveCurriculumLearning:\n",
    "    \"\"\" \n",
    "    Adaptive curriculum learning offers an object to store the accumulated rewards, \n",
    "    performances, difficulty socores, and probabilities for each pole length. \n",
    "    We check the last n (LOOK_BACK_WINDOW) rewards a pole has obtained and assign probabilities \n",
    "    to sample each length for an episode. The goal is to attack the policy on its weak spots, \n",
    "    we prioritize training on weak performing lengths. This should shift performance metrics and keeps \n",
    "    our attacks adaptive as we target weakest. \n",
    "    param: all_pole_lengths : numpy.ndarray - stores all the pole lengths\n",
    "    attr: p_adaptive : float - probability to select pole length from sample distribution, after uniform_episode_training_cap\n",
    "    attr: lb_window : int - look back window used to compute performance metric, number of recent pole length performances    \n",
    "    \"\"\"\n",
    "    LOOK_BACK_WINDOW = 20 # we only consider the last 20 rewards a pole has gotten (captures more sensitive information)\n",
    "    P_ADAPTIVE = 0.9 # probability of using adaptive probability distribution or uniform random\n",
    "    def __init__(self, all_pole_lengths):\n",
    "        self.all_pole_lengths = all_pole_lengths\n",
    "        self.rewards = defaultdict(list) # storing pole length episode reward for approach 2 (see 2.2)\n",
    "        self.performances = {} # keep track of performance metric for each pole length \n",
    "        self.difficulty_scores = {} # difficulty_scores\n",
    "        self.distribution = {} # probability distribution for pole lengths to sample from\n",
    "        \n",
    "        # initialize difficulties and probabilities (initially prob is uniform)\n",
    "        initial_prob = 1.0 / len(all_pole_lengths)\n",
    "        self.i_p = initial_prob\n",
    "        for length in all_pole_lengths:\n",
    "            self.difficulty_scores[length] = 1.0\n",
    "            self.distribution[length] = initial_prob\n",
    "\n",
    "    def update_rewards(self, pole_length, reward):\n",
    "        self.rewards[pole_length].append(reward)\n",
    "\n",
    "    def update_performances(self, pole_length):\n",
    "        \"\"\"\n",
    "        Here we update the performance of a pole, \n",
    "        \"\"\"\n",
    "        reward_list = self.rewards[pole_length]\n",
    "\n",
    "        # if a pole has not been played yet, it will be assigned a performance metric of 0\n",
    "        if not reward_list:\n",
    "            metric = 0\n",
    "        else:\n",
    "            # here we use a LOOK BACK WINDOW so that we dont use over-stabilized episode rewards\n",
    "            metric = np.mean(reward_list[-self.LOOK_BACK_WINDOW:])\n",
    "        self.performances[pole_length] = metric\n",
    "\n",
    "    def update_difficulties(self):\n",
    "        \"\"\"\n",
    "        Difficulties are inversely proportional to the performance metrics.\n",
    "        The worse a pole length has performed, the higher the diff score (diff=1 being the worst performing, diff=0 being best).\n",
    "        These require global updates as the update is relative to the totality of pole lengths.\n",
    "        Also normalizing + scaling the values down. Metrics can offer quite larger values otherwise. \n",
    "        \"\"\"\n",
    "        M_max = self.find_max()\n",
    "        M_min = self.find_min()\n",
    "        diff_M = M_max - M_min\n",
    "\n",
    "        # update all difficulty scores with new information\n",
    "        for pole_length, metric in self.performances.items():\n",
    "            if diff_M == 0:\n",
    "                difficulty = 1\n",
    "            else:\n",
    "                difficulty = 1 - ((metric-M_min) / diff_M) # if best perform, diff is 0 >>> probability assignment will be 0\n",
    "            self.difficulty_scores[pole_length] = difficulty\n",
    "    \n",
    "    def update_distribution(self):\n",
    "        \"\"\" \n",
    "        Distribution is also global, here we require an update proportional to the totality. \n",
    "        Normalizing the probabilities (between 0 and 1)\n",
    "        \"\"\"\n",
    "        if not self.difficulty_scores:\n",
    "            return\n",
    "        total_difficulty = sum(self.difficulty_scores.values())\n",
    "\n",
    "        if total_difficulty > 0:\n",
    "            for pole_length, difficulty in self.difficulty_scores.items():\n",
    "                self.distribution[pole_length] = difficulty / total_difficulty\n",
    "        else:\n",
    "            for pole_length in self.all_pole_lengths:\n",
    "                self.distribution[pole_length] = self.i_p\n",
    "    \n",
    "    def sample_length(self):\n",
    "        \"\"\"\n",
    "        Here select the pole length using either uniform or categorical prob distribution. \n",
    "        \"\"\"\n",
    "        if random.random() < self.P_ADAPTIVE:\n",
    "            pole_lengths = list(self.distribution.keys())\n",
    "            probs = list(self.distribution.values())\n",
    "\n",
    "            # if our prob dist is empty, we fallback to uniform selection\n",
    "            if not pole_lengths or sum(probs) == 0:\n",
    "                return np.random.choice(self.all_pole_lengths)\n",
    "            \n",
    "            # selection based on categorical sampling (discrete prob distribution)\n",
    "            return np.random.choice(a=pole_lengths, p=probs, size=1)[0]\n",
    "        else:\n",
    "            return np.random.choice(self.all_pole_lengths)\n",
    "\n",
    "    def find_max(self):\n",
    "        \"\"\"\n",
    "        Find max performing pole length\n",
    "        \"\"\"\n",
    "        if not self.performances:\n",
    "            return 0\n",
    "        return max(self.performances.values())\n",
    "\n",
    "    def find_min(self):\n",
    "        \"\"\"\n",
    "        Find min performing pole length\n",
    "        \"\"\"\n",
    "        if not self.performances:\n",
    "            return 0\n",
    "        return min(self.performances.values())\n",
    "    \n",
    "    def calculate_pole_stats(self):\n",
    "        \"\"\"\n",
    "        Nice display function for avg reward of each pole. Not functionally important at all. \n",
    "        \"\"\"\n",
    "        avg_pole_stats = {}\n",
    "        \n",
    "        for pole_length, rewards_list in self.rewards.items():\n",
    "            # Only process if the list is not empty\n",
    "            if rewards_list:\n",
    "                avg_reward = np.mean(rewards_list)\n",
    "                count = len(rewards_list)\n",
    "            else:\n",
    "                avg_reward = 0.0\n",
    "                count = 0\n",
    "            avg_pole_stats[pole_length] = {\n",
    "                \"average_reward\": avg_reward,\n",
    "                \"episode_count\": count\n",
    "            }\n",
    "        for p, avg in avg_pole_stats.items():\n",
    "            print(f\"Pole length {p} has avg reward {avg}\")\n",
    "        return avg_pole_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79d4dd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, policy_net, epsilon, action_dim):\n",
    "    \"\"\"\n",
    "    Select action using epsilon-greedy policy - did it with epsilon-greedy because of Assignent 1\n",
    "    \"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(action_dim)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_values = policy_net(state_tensor)\n",
    "            return q_values.argmax().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a686ab8",
   "metadata": {},
   "source": [
    "# Baseline model (A) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfbf697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_q_learning(epsilon, gamma, alpha, q_network, n_episodes, \n",
    "                    pole_lengths=None, env_name='CartPole-v1',\n",
    "                    batch_size=64, buffer_capacity=50000, \n",
    "                    update_target_every=1000, update_weights_every=4, \n",
    "                    epsilon_decay=1, epsilon_min=0.01,\n",
    "                    per_alpha=0.6, per_beta_init=0.4\n",
    "                    ):\n",
    "    \"\"\"\n",
    "    Deep q learning agent for CartPole-v1 environment with varying pole lengths.\n",
    "    \n",
    "    param: epsilon : float - initial exploration rate\n",
    "    param: gamma : float - discount factor\n",
    "    param: alpha : float - learning rate\n",
    "    param: q_network : QNetwork or None - pre-initialized network or None to create new one\n",
    "    param: n_episodes : int - number of training episodes\n",
    "    param: pole_lengths : array-like or None - array of pole lengths to train on (default: linspace(0.4, 1.8, 30))\n",
    "    param: env_name : str - gym environment name\n",
    "    param: batch_size : int - batch size for training\n",
    "    param: buffer_capacity : int - replay buffer capacity\n",
    "    param: update_target_every : int - how often to update target network\n",
    "    param: epsilon_decay : float - epsilon decay rate per episode\n",
    "    param: epsilon_min : float - minimum epsilon value\n",
    "\n",
    "    return: tuple : (policy_net, target_net, episode_returns)\n",
    "        - trained networks and list of episode rewards\n",
    "    \"\"\"\n",
    "\n",
    "    # initialization of environment\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    # initialization of networks if not provided\n",
    "    if q_network is None:\n",
    "        policy_net = QNetwork(state_dim, action_dim)\n",
    "        target_net = QNetwork(state_dim, action_dim)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        target_net.eval()\n",
    "    else:\n",
    "        policy_net = q_network\n",
    "        target_net = QNetwork(state_dim, action_dim)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        target_net.eval()\n",
    "\n",
    "    # initialization of optimizer\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=alpha)\n",
    "\n",
    "    # initialization of replay buffer\n",
    "    replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "    \n",
    "    # pole lengths for training\n",
    "    if pole_lengths is None:\n",
    "        pole_lengths = np.linspace(0.4, 1.8, 30)\n",
    "\n",
    "    # storing episode returns for plotting\n",
    "    episode_returns = []\n",
    "\n",
    "    # storing p lengths\n",
    "    visited_pole_lengths = []\n",
    "    \n",
    "    # copy of current epsilon value for decay\n",
    "    epsi = epsilon\n",
    "\n",
    "    # set counts to keep track for updating the weights/ and target net\n",
    "    count = 0 \n",
    "    \n",
    "    # training loop\n",
    "    for episode in range(n_episodes):\n",
    "        # randomly select pole length for this episode (we need to figure an experimental setup)\n",
    "        pole_length = np.random.choice(pole_lengths)\n",
    "        env.unwrapped.length = pole_length\n",
    "        \n",
    "        # reset environment\n",
    "        state = env.reset()[0]\n",
    "        episode_reward = 0.0\n",
    "        \n",
    "        # episode loop (1 episode = 1 pole length)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            \n",
    "            # increment count\n",
    "            count += 1\n",
    "\n",
    "            # select action\n",
    "            action = select_action(state, policy_net, epsi, action_dim)\n",
    "            \n",
    "            # take step\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # epsilon decay\n",
    "            if epsi > epsilon_min:\n",
    "                epsi = max(epsilon_min, epsi * epsilon_decay)\n",
    "\n",
    "            # store transition in replay buffer\n",
    "            replay_buffer.push(state, action, reward, next_state, float(done), pole_length)\n",
    "\n",
    "            # deep q learning update (using mini-batch from replay buffer)\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                # sample batch from replay buffer\n",
    "                if count % update_weights_every == 0:\n",
    "                    batch = replay_buffer.sample(batch_size) \n",
    "                    states, actions, rewards, next_states, dones, labels_pole_length = zip(*batch)\n",
    "\n",
    "                    # store the pole lengths in our self.sampled p lengths for later use\n",
    "                    replay_buffer.sampled_pole_lengths.extend(labels_pole_length)\n",
    "\n",
    "                    # convert to tensors \n",
    "                    states_t = torch.FloatTensor(states)\n",
    "                    actions_t = torch.LongTensor(actions).unsqueeze(1)\n",
    "                    rewards_t = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "                    next_states_t = torch.FloatTensor(next_states)\n",
    "                    dones_t = torch.FloatTensor(dones).unsqueeze(1)\n",
    "\n",
    "                    #get current q values\n",
    "                    current_q = policy_net(states_t).gather(1, actions_t)\n",
    "                    \n",
    "                    # target values\n",
    "                    with torch.no_grad():\n",
    "                        next_max = target_net(next_states_t).max(1)[0].unsqueeze(1)\n",
    "                        td_target = rewards_t + gamma * next_max * (1 - dones_t)\n",
    "                    \n",
    "                    # loss calc\n",
    "                    loss = nn.MSELoss()(current_q, td_target)\n",
    "                    \n",
    "                    # backprop and optimize + gradient clipping\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)\n",
    "                    optimizer.step()\n",
    "            \n",
    "            # update target network periodically\n",
    "            if count % update_target_every == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        #store episode reward\n",
    "        episode_returns.append(episode_reward)\n",
    "\n",
    "        # store pole legnth\n",
    "        visited_pole_lengths.append(pole_length)\n",
    "        \n",
    "        #only for seeing the progress\n",
    "        if episode % 100 == 0:\n",
    "            avg_reward = np.mean(episode_returns[-100:]) if len(episode_returns) >= 100 else np.mean(episode_returns)\n",
    "            print(f\"Episode {episode}/{n_episodes} | \"\n",
    "                  f\"Avg Reward: {avg_reward:.1f} | \"\n",
    "                  f\"Epsilon: {epsi:.3f}\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "    sampled_pole_lengths = replay_buffer.sampled_pole_lengths\n",
    "    \n",
    "    return (policy_net, target_net, episode_returns, sampled_pole_lengths, visited_pole_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4490c92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/2000 | Avg Reward: 18.0 | Epsilon: 0.100\n",
      "Episode 100/2000 | Avg Reward: 19.5 | Epsilon: 0.100\n",
      "Episode 200/2000 | Avg Reward: 14.1 | Epsilon: 0.100\n",
      "Episode 300/2000 | Avg Reward: 17.6 | Epsilon: 0.100\n",
      "Episode 400/2000 | Avg Reward: 183.7 | Epsilon: 0.100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m torch.manual_seed(seed)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# --- training test ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m policy_net, target_net, returns, sampled_pole_lengths, visited_pole_lengths = \u001b[43mdeep_q_learning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mq_network\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpole_lengths\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mCartPole-v1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbuffer_capacity\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mupdate_target_every\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mupdate_weights_every\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepsilon_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepsilon_min\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_alpha\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_beta_init\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.4\u001b[39;49m\n\u001b[32m     28\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m----finished training----\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLast 3 episode returns:\u001b[39m\u001b[33m\"\u001b[39m, returns[-\u001b[32m3\u001b[39m:] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(returns) >= \u001b[32m3\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m returns)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 143\u001b[39m, in \u001b[36mdeep_q_learning\u001b[39m\u001b[34m(epsilon, gamma, alpha, q_network, n_episodes, pole_lengths, env_name, batch_size, buffer_capacity, update_target_every, update_weights_every, epsilon_decay, epsilon_min, per_alpha, per_beta_init)\u001b[39m\n\u001b[32m    140\u001b[39m replay_buffer.update_priorities(indices, td_error) \n\u001b[32m    142\u001b[39m \u001b[38;5;66;03m# backprop and optimize + gradient clipping\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m loss.backward()\n\u001b[32m    145\u001b[39m torch.nn.utils.clip_grad_norm_(policy_net.parameters(), \u001b[32m1.0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\coding-software\\conda\\miniconda\\envs\\introrl25\\Lib\\site-packages\\torch\\_compile.py:53\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     50\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive, wrapping=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     51\u001b[39m     fn.__dynamo_disable = disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\coding-software\\conda\\miniconda\\envs\\introrl25\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:929\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    927\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m.callback))\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m929\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    931\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\coding-software\\conda\\miniconda\\envs\\introrl25\\Lib\\site-packages\\torch\\optim\\optimizer.py:1027\u001b[39m, in \u001b[36mOptimizer.zero_grad\u001b[39m\u001b[34m(self, set_to_none)\u001b[39m\n\u001b[32m   1024\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1025\u001b[39m     per_device_and_dtype_grads = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1027\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprofiler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_zero_grad_profile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_groups\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparams\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\coding-software\\conda\\miniconda\\envs\\introrl25\\Lib\\site-packages\\torch\\autograd\\profiler.py:776\u001b[39m, in \u001b[36mrecord_function.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    775\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m     \u001b[38;5;28mself\u001b[39m.record = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprofiler\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_record_function_enter_new\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    779\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\coding-software\\conda\\miniconda\\envs\\introrl25\\Lib\\site-packages\\torch\\_ops.py:1243\u001b[39m, in \u001b[36mOpOverloadPacket.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1241\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[32m   1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1243\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#Test the deep q learning function\n",
    "# --- IGNORE ---\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# --- training test ---\n",
    "policy_net, target_net, returns, sampled_pole_lengths, visited_pole_lengths = deep_q_learning(\n",
    "    epsilon= 0.1, \n",
    "    gamma=0.99, \n",
    "    alpha=1e-3, \n",
    "    q_network=None, \n",
    "    n_episodes=2000, \n",
    "    pole_lengths=None, \n",
    "    env_name='CartPole-v1',\n",
    "    batch_size=64, \n",
    "    buffer_capacity=50000, \n",
    "    update_target_every=1000, \n",
    "    update_weights_every=4, \n",
    "    epsilon_decay=1, \n",
    "    epsilon_min=0.05,\n",
    "    per_alpha=0.6, \n",
    "    per_beta_init=0.4\n",
    ")\n",
    "\n",
    "print(\"----finished training----\")\n",
    "print(\"Last 3 episode returns:\", returns[-3:] if len(returns) >= 3 else returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c38a3",
   "metadata": {},
   "source": [
    "`quick check that pole lengths are being returned well`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420d88cc",
   "metadata": {},
   "source": [
    "`saving the training data and model`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bafabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List successfully saved to trained_nn_version_2\\baseline_5k_rewards.json\n",
      "List successfully saved to trained_nn_version_2\\baseline_5k_sampled_pole_lengths.json\n"
     ]
    }
   ],
   "source": [
    "SAVE_FOLDER = \"trained_models_rb_strategy\"\n",
    "MODEL_NAME = \"baseline.pth\" # .pth or .pt are common extensions\n",
    "SAVE_PATH = os.path.join(SAVE_FOLDER, MODEL_NAME)\n",
    "if not os.path.exists(SAVE_FOLDER):\n",
    "    os.makedirs(SAVE_FOLDER)\n",
    "    print(f\"Created directory: {SAVE_FOLDER}\")\n",
    "\n",
    "torch.save(policy_net.state_dict(), SAVE_PATH)\n",
    "\n",
    "_SAVE_FOLDER = \"training_data_version_2\"\n",
    "_DATA_NAME = \"baseline_rewards.json\" \n",
    "__DATA_NAME = \"baseline_sampled_pole_lengths.json\"# .pth or .pt are common extensions\n",
    "os.makedirs(_SAVE_FOLDER, exist_ok=True)\n",
    "full_path = os.path.join(_SAVE_FOLDER, _DATA_NAME)\n",
    "_full_path = os.path.join(_SAVE_FOLDER, __DATA_NAME)\n",
    "try:\n",
    "    with open(full_path, 'w') as file:\n",
    "        json.dump(returns, file, indent=4) # indent=4 makes the file nicely formatted\n",
    "    print(f\"List successfully saved to {full_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "try:\n",
    "    with open(_full_path, 'w') as file:\n",
    "        json.dump(sampled_pole_lengths, file, indent=4) # indent=4 makes the file nicely formatted\n",
    "    print(f\"List successfully saved to {_full_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "_visited_pole_lengths = \"baseline_visited.json\"\n",
    "v_path = os.path.join(_SAVE_FOLDER, _visited_pole_lengths)\n",
    "\n",
    "try:\n",
    "    with open(v_path, 'w') as file:\n",
    "        json.dump(visited_pole_lengths, file, indent=4) # indent=4 makes the file nicely formatted\n",
    "    print(f\"List successfully saved to {v_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc94b516",
   "metadata": {},
   "source": [
    "# Prioritized Experience Replay strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01094341",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def deep_q_learning(epsilon, gamma, alpha, q_network, n_episodes, \n",
    "                    pole_lengths=None, env_name='CartPole-v1',\n",
    "                    batch_size=64, buffer_capacity=50000, \n",
    "                    update_target_every=1000, update_weights_every=4, \n",
    "                    epsilon_decay=0.995, epsilon_min=0.01,\n",
    "                    per_alpha=0.6, per_beta_init=0.4\n",
    "                    ):\n",
    "    \"\"\"\n",
    "    Deep q learning agent for CartPole-v1 environment with varying pole lengths.\n",
    "    \n",
    "    param: epsilon : float - initial exploration rate\n",
    "    param: gamma : float - discount factor\n",
    "    param: alpha : float - learning rate\n",
    "    param: q_network : QNetwork or None - pre-initialized network or None to create new one\n",
    "    param: n_episodes : int - number of training episodes\n",
    "    param: pole_lengths : array-like or None - array of pole lengths to train on (default: linspace(0.4, 1.8, 30))\n",
    "    param: env_name : str - gym environment name\n",
    "    param: batch_size : int - batch size for training\n",
    "    param: buffer_capacity : int - replay buffer capacity\n",
    "    param: update_target_every : int - how often to update target network\n",
    "    param: epsilon_decay : float - epsilon decay rate per episode\n",
    "    param: epsilon_min : float - minimum epsilon value\n",
    "\n",
    "    return: tuple : (policy_net, target_net, episode_returns)\n",
    "        - trained networks and list of episode rewards\n",
    "    \"\"\"\n",
    "\n",
    "    # initialization of environment\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    # initialization of networks if not provided\n",
    "    if q_network is None:\n",
    "        policy_net = QNetwork(state_dim, action_dim)\n",
    "        target_net = QNetwork(state_dim, action_dim)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        target_net.eval()\n",
    "    else:\n",
    "        policy_net = q_network\n",
    "        target_net = QNetwork(state_dim, action_dim)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        target_net.eval()\n",
    "\n",
    "    # initialization of optimizer\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=alpha)\n",
    "\n",
    "    # initialization of replay buffer\n",
    "    replay_buffer = PrioritizedReplayBuffer(buffer_capacity, per_alpha)\n",
    "    \n",
    "    # pole lengths for training\n",
    "    if pole_lengths is None:\n",
    "        pole_lengths = np.linspace(0.4, 1.8, 30)\n",
    "\n",
    "    # storing episode returns for plotting\n",
    "    episode_returns = []\n",
    "\n",
    "    # storing pole lengths \n",
    "    visited_pole_lengths = []\n",
    "    \n",
    "    # copy of current epsilon value for decay\n",
    "    epsi = epsilon\n",
    "\n",
    "    # set counts to keep track for updating the weights/ and target net\n",
    "    count = 0 \n",
    "\n",
    "    total_steps = n_episodes * 500 # Estimate max steps for annealing (CartPole max steps is 500)\n",
    "    beta = per_beta_init\n",
    "    beta_increment = (1.0 - per_beta_init) / total_steps # Anneal beta to 1.0 over training\n",
    "\n",
    "    \n",
    "    # training loop\n",
    "    for episode in range(n_episodes):\n",
    "        # randomly select pole length for this episode (we need to figure an experimental setup)\n",
    "        pole_length = np.random.choice(pole_lengths)\n",
    "        env.unwrapped.length = pole_length\n",
    "        \n",
    "        # reset environment\n",
    "        state = env.reset()[0]\n",
    "        episode_reward = 0.0\n",
    "        \n",
    "        # episode loop (1 episode = 1 pole length)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            \n",
    "            # increment count\n",
    "            count += 1\n",
    "\n",
    "            # Anneal Beta for Importance Sampling (IS) weights\n",
    "            beta = min(1.0, beta + beta_increment) \n",
    "\n",
    "            # select action\n",
    "            action = select_action(state, policy_net, epsi, action_dim)\n",
    "            \n",
    "            # take step\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # epsilon decay\n",
    "            if epsi > epsilon_min:\n",
    "                epsi = max(epsilon_min, epsi * epsilon_decay)\n",
    "\n",
    "            # store transition in replay buffer\n",
    "            replay_buffer.push(state, action, reward, next_state, float(done), pole_length)\n",
    "\n",
    "            # deep q learning update (using mini-batch from replay buffer)\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                # sample batch from replay buffer\n",
    "                if count % update_weights_every == 0:\n",
    "                    batch, indices, weights = replay_buffer.sample(batch_size, beta) \n",
    "                    states, actions, rewards, next_states, dones, labels_pole_length = zip(*batch)\n",
    "\n",
    "                    # store the pole lengths in our self.sampled p lengths for later use\n",
    "                    replay_buffer.sampled_pole_lengths.extend(labels_pole_length)\n",
    "\n",
    "                    # convert to tensors \n",
    "                    states_t = torch.FloatTensor(states)\n",
    "                    actions_t = torch.LongTensor(actions).unsqueeze(1)\n",
    "                    rewards_t = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "                    next_states_t = torch.FloatTensor(next_states)\n",
    "                    dones_t = torch.FloatTensor(dones).unsqueeze(1)\n",
    "                    weights_t = torch.FloatTensor(weights).unsqueeze(1) \n",
    "\n",
    "                    #get current q values\n",
    "                    current_q = policy_net(states_t).gather(1, actions_t)\n",
    "                    \n",
    "                    # target values\n",
    "                    with torch.no_grad():\n",
    "                        next_max = target_net(next_states_t).max(1)[0].unsqueeze(1)\n",
    "                        td_target = rewards_t + gamma * next_max * (1 - dones_t)\n",
    "                        td_error = torch.abs(td_target - current_q).squeeze().detach().cpu().numpy()\n",
    "\n",
    "                    \n",
    "                    # loss calc\n",
    "                    loss_unweighted = torch.nn.MSELoss(reduction='none')(current_q, td_target)\n",
    "                    loss = (loss_unweighted * weights_t).mean()\n",
    "\n",
    "                    # update priorities in the replay buffer\n",
    "                    replay_buffer.update_priorities(indices, td_error) \n",
    "                    \n",
    "                    # backprop and optimize + gradient clipping\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)\n",
    "                    optimizer.step()\n",
    "            \n",
    "            # update target network periodically\n",
    "            if count % update_target_every == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        #store episode reward\n",
    "        episode_returns.append(episode_reward)\n",
    "        \n",
    "        # store pole legnth\n",
    "        visited_pole_lengths.append(pole_length)\n",
    "\n",
    "        #only for seeing the progress\n",
    "        if episode % 100 == 0:\n",
    "            avg_reward = np.mean(episode_returns[-100:]) if len(episode_returns) >= 100 else np.mean(episode_returns)\n",
    "            print(f\"Episode {episode}/{n_episodes} | \"\n",
    "                  f\"Avg Reward: {avg_reward:.1f} | \"\n",
    "                  f\"Epsilon: {epsi:.3f}\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "    sampled_pole_lengths = replay_buffer.sampled_pole_lengths\n",
    "    \n",
    "    return (policy_net, target_net, episode_returns, sampled_pole_lengths, visited_pole_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee846bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/2000 | Avg Reward: 18.0 | Epsilon: 0.100\n",
      "Episode 100/2000 | Avg Reward: 19.0 | Epsilon: 0.100\n",
      "Episode 200/2000 | Avg Reward: 18.3 | Epsilon: 0.100\n",
      "Episode 300/2000 | Avg Reward: 82.2 | Epsilon: 0.100\n",
      "Episode 400/2000 | Avg Reward: 265.8 | Epsilon: 0.100\n",
      "Episode 500/2000 | Avg Reward: 207.1 | Epsilon: 0.100\n",
      "Episode 600/2000 | Avg Reward: 129.6 | Epsilon: 0.100\n",
      "Episode 700/2000 | Avg Reward: 137.8 | Epsilon: 0.100\n",
      "Episode 800/2000 | Avg Reward: 191.0 | Epsilon: 0.100\n",
      "Episode 900/2000 | Avg Reward: 347.7 | Epsilon: 0.100\n",
      "Episode 1000/2000 | Avg Reward: 192.0 | Epsilon: 0.100\n",
      "Episode 1100/2000 | Avg Reward: 245.8 | Epsilon: 0.100\n",
      "Episode 1200/2000 | Avg Reward: 359.9 | Epsilon: 0.100\n",
      "Episode 1300/2000 | Avg Reward: 417.4 | Epsilon: 0.100\n",
      "Episode 1400/2000 | Avg Reward: 297.1 | Epsilon: 0.100\n",
      "Episode 1500/2000 | Avg Reward: 230.4 | Epsilon: 0.100\n",
      "Episode 1600/2000 | Avg Reward: 206.0 | Epsilon: 0.100\n",
      "Episode 1700/2000 | Avg Reward: 388.7 | Epsilon: 0.100\n",
      "Episode 1800/2000 | Avg Reward: 316.5 | Epsilon: 0.100\n",
      "Episode 1900/2000 | Avg Reward: 371.3 | Epsilon: 0.100\n",
      "----finished training----\n",
      "Last 3 episode returns: [500.0, 443.0, 60.0]\n"
     ]
    }
   ],
   "source": [
    "#Test the deep q learning function\n",
    "# --- IGNORE ---\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# --- training test ---\n",
    "policy_net, target_net, returns, sampled_pole_lengths, visited_pole_lengths = deep_q_learning(\n",
    "    epsilon= 0.1, \n",
    "    gamma=0.99, \n",
    "    alpha=1e-3, \n",
    "    q_network=None, \n",
    "    n_episodes=2000, \n",
    "    pole_lengths=None, \n",
    "    env_name='CartPole-v1',\n",
    "    batch_size=64, \n",
    "    buffer_capacity=50000, \n",
    "    update_target_every=1000, \n",
    "    update_weights_every=4, \n",
    "    epsilon_decay=1, \n",
    "    epsilon_min=0.05,\n",
    "    per_alpha=0.6, \n",
    "    per_beta_init=0.4\n",
    ")\n",
    "\n",
    "print(\"----finished training----\")\n",
    "print(\"Last 3 episode returns:\", returns[-3:] if len(returns) >= 3 else returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6ac072",
   "metadata": {},
   "source": [
    "`saving training data and model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5916a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List successfully saved to training_data_version_2\\per_strategy_rewards.json\n",
      "List successfully saved to training_data_version_2\\per_strategy_rewards_sampled_pole_lengths.json\n",
      "List successfully saved to training_data_version_2\\per_visited.json\n"
     ]
    }
   ],
   "source": [
    "SAVE_FOLDER = \"trained_nn_version_2\"\n",
    "MODEL_NAME = \"per_strategy.pth\" # .pth or .pt are common extensions\n",
    "SAVE_PATH = os.path.join(SAVE_FOLDER, MODEL_NAME)\n",
    "if not os.path.exists(SAVE_FOLDER):\n",
    "    os.makedirs(SAVE_FOLDER)\n",
    "    print(f\"Created directory: {SAVE_FOLDER}\")\n",
    "\n",
    "torch.save(policy_net.state_dict(), SAVE_PATH)\n",
    "\n",
    "_SAVE_FOLDER = \"training_data_version_2\"\n",
    "_DATA_NAME = \"per_strategy_rewards.json\" \n",
    "__DATA_NAME = \"per_strategy_rewards_sampled_pole_lengths.json\"# .pth or .pt are common extensions\n",
    "os.makedirs(_SAVE_FOLDER, exist_ok=True)\n",
    "full_path = os.path.join(_SAVE_FOLDER, _DATA_NAME)\n",
    "_full_path = os.path.join(_SAVE_FOLDER, __DATA_NAME)\n",
    "try:\n",
    "    with open(full_path, 'w') as file:\n",
    "        json.dump(returns, file, indent=4) # indent=4 makes the file nicely formatted\n",
    "    print(f\"List successfully saved to {full_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "try:\n",
    "    with open(_full_path, 'w') as file:\n",
    "        json.dump(sampled_pole_lengths, file, indent=4) # indent=4 makes the file nicely formatted\n",
    "    print(f\"List successfully saved to {_full_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "_visited_pole_lengths = \"per_strategy_visited.json\"\n",
    "v_path = os.path.join(_SAVE_FOLDER, _visited_pole_lengths)\n",
    "\n",
    "try:\n",
    "    with open(v_path, 'w') as file:\n",
    "        json.dump(visited_pole_lengths, file, indent=4) # indent=4 makes the file nicely formatted\n",
    "    print(f\"List successfully saved to {v_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a581b135",
   "metadata": {},
   "source": [
    "# Adaptive Curriculum Learning strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe5caf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_q_learning(epsilon, gamma, alpha, q_network, n_episodes,\n",
    "                    uniform_episode_training_cap,\n",
    "                    pole_lengths=None, env_name='CartPole-v1',\n",
    "                    batch_size=64, buffer_capacity=50000, \n",
    "                    update_target_every=1000, update_weights_every=4,\n",
    "                    epsilon_decay=1, \n",
    "                    epsilon_min=0.05):\n",
    "    \"\"\"\n",
    "    Deep q learning agent for CartPole-v1 environment with varying pole lengths.\n",
    "    \n",
    "    param: epsilon : float - initial exploration rate\n",
    "    param: gamma : float - discount factor\n",
    "    param: alpha : float - learning rate\n",
    "    param: q_network : QNetwork or None - pre-initialized network or None to create new one\n",
    "    param: n_episodes : int - number of training episodes\n",
    "    param: uniform_episode_training_cap : Union[int, None] - number episodes trained with uniform length selection\n",
    "    if None, no adaptive curriculum learning enabled\n",
    "    param: pole_lengths : array-like or None - array of pole lengths to train on (default: linspace(0.4, 1.8, 30))\n",
    "    param: env_name : str - gym environment name\n",
    "    param: batch_size : int - batch size for training\n",
    "    param: buffer_capacity : int - replay buffer capacity\n",
    "    param: update_target_every : int - how often to update target network\n",
    "    param: epsilon_decay : float - epsilon decay rate per episode\n",
    "    param: epsilon_min : float - minimum epsilon value\n",
    "\n",
    "    return: tuple : (policy_net, target_net, episode_returns)\n",
    "        - trained networks and list of episode rewards\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialization of environment\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    # initialization of networks if not provided\n",
    "    if q_network is None:\n",
    "        policy_net = QNetwork(state_dim, action_dim)\n",
    "        target_net = QNetwork(state_dim, action_dim)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        target_net.eval()\n",
    "    else:\n",
    "        policy_net = q_network\n",
    "        target_net = QNetwork(state_dim, action_dim)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        target_net.eval()\n",
    "\n",
    "    # initialization of optimizer\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=alpha)\n",
    "\n",
    "    # initialization of replay buffer\n",
    "    replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "    \n",
    "    # pole lengths for training\n",
    "    if pole_lengths is None:\n",
    "        pole_lengths = np.linspace(0.4, 1.8, 30)\n",
    "\n",
    "    # storing episode returns for plotting\n",
    "    episode_returns = []\n",
    "\n",
    "    # storing visited pole lengths \n",
    "    visited_pole_lengths = []\n",
    "\n",
    "    # initialize the acl class for adaptive hyperparametre sampling\n",
    "    acl = AdaptiveCurriculumLearning(pole_lengths)\n",
    "\n",
    "    # copy of current epsilon value for decay\n",
    "    epsi = epsilon\n",
    "\n",
    "    # set counts to keep track for updating the weights/ and target net\n",
    "    count = 0\n",
    "    \n",
    "    # training loop\n",
    "    for episode in range(n_episodes):\n",
    "        # if current episode is below the uniform_episode_training_cap we select from a uniform distr\n",
    "        if episode <= uniform_episode_training_cap: \n",
    "            pole_length = np.random.choice(pole_lengths)\n",
    "        # else use adaptive probability distrubition \n",
    "        else: \n",
    "            pole_length = acl.sample_length()\n",
    "\n",
    "        env.unwrapped.length = pole_length\n",
    "        \n",
    "        # reset environment\n",
    "        state = env.reset()[0]\n",
    "        episode_reward = 0.0\n",
    "        \n",
    "        # episode loop (1 episode = 1 pole length)\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "\n",
    "            # increment count\n",
    "            count += 1\n",
    "\n",
    "            # select action\n",
    "            action = select_action(state, policy_net, epsi, action_dim)\n",
    "            \n",
    "            # take step\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # store transition in replay buffer\n",
    "            replay_buffer.push(state, action, reward, next_state, float(done), pole_length)\n",
    "\n",
    "            # epsilon decay\n",
    "            if epsi > epsilon_min:\n",
    "                epsi = max(epsilon_min, epsi * epsilon_decay)\n",
    "\n",
    "            # deep q learning update (using mini-batch from replay buffer)\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                # sample batch from replay buffer\n",
    "                if count % update_weights_every == 0:\n",
    "                    batch = replay_buffer.sample(batch_size)\n",
    "                    states, actions, rewards, next_states, dones, labels_pole_length = zip(*batch)\n",
    "\n",
    "                    # add pole length to buffer for later use\n",
    "                    replay_buffer.sampled_pole_lengths.extend(labels_pole_length)\n",
    "                    \n",
    "                    # convert to tensors \n",
    "                    states_t = torch.FloatTensor(states)\n",
    "                    actions_t = torch.LongTensor(actions).unsqueeze(1)\n",
    "                    rewards_t = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "                    next_states_t = torch.FloatTensor(next_states)\n",
    "                    dones_t = torch.FloatTensor(dones).unsqueeze(1)\n",
    "\n",
    "                    #get current q values\n",
    "                    current_q = policy_net(states_t).gather(1, actions_t)\n",
    "                    \n",
    "                    # target values\n",
    "                    with torch.no_grad():\n",
    "                        next_max = target_net(next_states_t).max(1)[0].unsqueeze(1)\n",
    "                        td_target = rewards_t + gamma * next_max * (1 - dones_t)\n",
    "                    \n",
    "                    # loss calc\n",
    "                    loss = nn.MSELoss()(current_q, td_target)\n",
    "                    \n",
    "                    # backprop and optimize + gradient clipping\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)\n",
    "                    optimizer.step()\n",
    "\n",
    "            # update target network periodically (every n actions)\n",
    "            if count % update_target_every == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        # adding pole length\n",
    "        visited_pole_lengths.append(pole_length)\n",
    "\n",
    "        #store episode reward\n",
    "        episode_returns.append(episode_reward)\n",
    "\n",
    "\n",
    "        \n",
    "        # update acl for rewards, performance metrics, scores, and probs\n",
    "        # difficulty scores and probabilities are global updates, can be seen in the respective update methods\n",
    "        acl.update_rewards(pole_length, episode_reward)\n",
    "        if episode >= uniform_episode_training_cap:\n",
    "            acl.update_performances(pole_length)\n",
    "            acl.update_difficulties()\n",
    "            acl.update_distribution()\n",
    "\n",
    "        #only for seeing the progress\n",
    "        if episode % 100 == 0:\n",
    "            avg_reward = np.mean(episode_returns[-100:]) if len(episode_returns) >= 100 else np.mean(episode_returns)\n",
    "            print(f\"Episode {episode}/{n_episodes} | \"\n",
    "                  f\"Avg Reward: {avg_reward:.1f} | \"\n",
    "                  f\"Epsilon: {epsi:.3f} | \"\n",
    "                  )            \n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    return (policy_net, target_net, episode_returns, replay_buffer.sampled_pole_lengths, visited_pole_lengths, (acl.rewards, acl.difficulty_scores, acl.distribution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd42e076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the deep q learning function\n",
    "# --- IGNORE ---\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# --- training test ---\n",
    "policy_net, target_net, returns, sampled_pole_lengths, visited_pole_lengths, acl_data = deep_q_learning(\n",
    "    epsilon= 0.1, \n",
    "    gamma=0.99, \n",
    "    alpha=1e-3, \n",
    "    q_network=None, \n",
    "    n_episodes=5000, \n",
    "    uniform_episode_training_cap=3000,\n",
    "    pole_lengths=None, \n",
    "    env_name='CartPole-v1',\n",
    "    batch_size=64, \n",
    "    buffer_capacity=50000, \n",
    "    update_target_every=1000, \n",
    "    update_weights_every=4, \n",
    "    epsilon_decay=1, \n",
    "    epsilon_min=0.05\n",
    ")\n",
    "\n",
    "print(\"----finished training----\")\n",
    "print(\"Last 3 episode returns:\", returns[-3:] if len(returns) >= 3 else returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815f41e3",
   "metadata": {},
   "source": [
    "`saving training data and model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cb0db4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'acl_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m _DATA_NAME = \u001b[33m\"\u001b[39m\u001b[33macl_strategy_rewards.json\u001b[39m\u001b[33m\"\u001b[39m \n\u001b[32m     12\u001b[39m __DATA_NAME = \u001b[33m\"\u001b[39m\u001b[33macl_strategy_rewards_sampled_pole_lengths.json\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;66;03m# .pth or .pt are common extensions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m acl.rewards, acl.difficulty_scores, acl.distribution = \u001b[43macl_data\u001b[49m\n\u001b[32m     14\u001b[39m ACL_REWARDS\n\u001b[32m     15\u001b[39m ACL_SCORES\n",
      "\u001b[31mNameError\u001b[39m: name 'acl_data' is not defined"
     ]
    }
   ],
   "source": [
    "SAVE_FOLDER = \"trained_nn_version_2\"\n",
    "MODEL_NAME = \"per_strategy.pth\" # .pth or .pt are common extensions\n",
    "SAVE_PATH = os.path.join(SAVE_FOLDER, MODEL_NAME)\n",
    "if not os.path.exists(SAVE_FOLDER):\n",
    "    os.makedirs(SAVE_FOLDER)\n",
    "    print(f\"Created directory: {SAVE_FOLDER}\")\n",
    "\n",
    "torch.save(policy_net.state_dict(), SAVE_PATH)\n",
    "\n",
    "_SAVE_FOLDER = \"trained_nn_version_2\"\n",
    "_DATA_NAME = \"acl_strategy_rewards.json\" \n",
    "__DATA_NAME = \"acl_strategy_rewards_sampled_pole_lengths.json\"# .pth or .pt are common extensions\n",
    "acl_rewards, acl_difficulty_scores, acl_distribution = acl_data\n",
    "# ACL_REWARDS = \"acl_strategy_rewards.json\"\n",
    "# ACL_SCORES = \"acl_strategy_scores.json\"\n",
    "# ACL_PROBABILITY = \"acl_strategy_distribution.json\"\n",
    "ACL_DATA = \"acl_strategy_data_tuple.json\"\n",
    "\n",
    "os.makedirs(_SAVE_FOLDER, exist_ok=True)\n",
    "full_path = os.path.join(_SAVE_FOLDER, _DATA_NAME)\n",
    "_full_path = os.path.join(_SAVE_FOLDER, __DATA_NAME)\n",
    "acl_rewards_path = os.path.join(_SAVE_FOLDER, ACL_DATA)\n",
    "try:\n",
    "    with open(full_path, 'w') as file:\n",
    "        json.dump(returns, file, indent=4) # indent=4 makes the file nicely formatted\n",
    "    print(f\"List successfully saved to {full_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "try:\n",
    "    with open(_full_path, 'w') as file:\n",
    "        json.dump(sampled_pole_lengths, file, indent=4) # indent=4 makes the file nicely formatted\n",
    "    print(f\"List successfully saved to {_full_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "try:\n",
    "    with open(acl_rewards_path, 'w') as file:\n",
    "        json.dump(acl_data, file, indent=4) # indent=4 makes the file nicely formatted\n",
    "    print(f\"List successfully saved to {acl_rewards_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78431433",
   "metadata": {},
   "source": [
    "# Testing with the test script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dff0b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_script import test_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3fcc14d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "test_script() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtest_script\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrained_nn_version_2/per_strategy.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: test_script() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "test_script(\"trained_nn_version_2/per_strategy.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc52e06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "introrl25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
