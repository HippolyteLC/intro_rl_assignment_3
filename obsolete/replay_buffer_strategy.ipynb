{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "464925d8",
   "metadata": {},
   "source": [
    "`1) IMPORT WHAT WE NEED`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cabb66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from test_script import QNetwork\n",
    "from test_script import bar_plot, test_pole_length, test_script\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe60ea6",
   "metadata": {},
   "source": [
    "`2a) STRATIFIED REPLAY BUFFER`\n",
    "\n",
    "This replay buffer uses (near) uniform sampling (given sufficient samples are available for each labelled sample). The samples are labelled by pole length, with the idea that we sample evenly for pole lengths and thus train evenly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2bb5d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class StratifiedReplayBuffer:\n",
    "    \"\"\"\n",
    "    Replay Buffer to store experience tuples for deep q learning.\n",
    "    The replay buffer stores experiences from many episodes and randomly samples them during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = {}\n",
    "        self.capacity = capacity\n",
    "    \n",
    "    def push(self, label, state, action, reward, next_state, done):\n",
    "        sample = (state, action, reward, next_state, done)\n",
    "        if label not in  self.buffer:\n",
    "            self.buffer[label] = deque()\n",
    "            num_labels = len(self.buffer)\n",
    "            new_max_buffer_capacity = self.capacity // num_labels\n",
    "            remainder = self.capacity % num_labels\n",
    "            # dynamically adapt deque max cap as we add sampled pole lengths to our superbuffer. \n",
    "            for i, existing_label in enumerate(self.buffer.keys()):\n",
    "                current_maxlen = new_max_buffer_capacity + (1 if i < remainder else 0)        \n",
    "                old_deque = self.buffer[existing_label]\n",
    "                self.buffer[existing_label] = deque(old_deque, maxlen=current_maxlen) \n",
    "\n",
    "\n",
    "        self.buffer[label].append(sample)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if not self.buffer:\n",
    "            return []\n",
    "        labels = list(self.buffer.keys()) \n",
    "        samples_per_label = batch_size//len(labels)\n",
    "        remainder = batch_size % len(labels)\n",
    "        all_samples = []\n",
    "        # iterate through all labels (pole lengths) to ensure most even distribution in sample (uniform)\n",
    "        for i, label in enumerate(labels):\n",
    "            label_buffer = self.buffer[label]\n",
    "            num_to_sample = samples_per_label + (1 if i < remainder else 0)\n",
    "            num_to_sample = min(num_to_sample, len(label_buffer))\n",
    "            batch = random.sample(label_buffer, num_to_sample)\n",
    "            all_samples.extend(batch)\n",
    "        \n",
    "        random.shuffle(all_samples)\n",
    "        return all_samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return sum(len(d) for d in self.buffer.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ae91ff",
   "metadata": {},
   "source": [
    "`2b) OUR ADAPTIVE CURRICULUM LEARNING FUNCTION (UNUSED)`\n",
    "\n",
    "Defined here to allow for use but it is not part of this module of the strategies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e12696cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveCurriculumLearning:\n",
    "    \"\"\" \n",
    "    Adaptive curriculum learning offers an object to store the accumulated rewards, \n",
    "    performances, difficulty socores, and probabilities for each pole length. \n",
    "    We check the last n (LOOK_BACK_WINDOW) rewards a pole has obtained and assign probabilities \n",
    "    to sample each length for an episode. The goal is to attack the policy on its weak spots, \n",
    "    we prioritize training on weak performing lengths. This should shift performance metrics and keeps \n",
    "    our attacks adaptive as we target weakest. \n",
    "    param: all_pole_lengths : numpy.ndarray - stores all the pole lengths\n",
    "    attr: p_adaptive : float - probability to select pole length from sample distribution, after uniform_episode_training_cap\n",
    "    attr: lb_window : int - look back window used to compute performance metric, number of recent pole length performances    \n",
    "    \"\"\"\n",
    "    LOOK_BACK_WINDOW = 20 # we only consider the last 20 rewards a pole has gotten (captures more sensitive information)\n",
    "    P_ADAPTIVE = 0.95 # probability of using adaptive probability distribution or uniform random\n",
    "    def __init__(self, all_pole_lengths):\n",
    "        self.all_pole_lengths = all_pole_lengths\n",
    "        self.rewards = defaultdict(list) # storing pole length episode reward for approach 2 (see 2.2)\n",
    "        self.performances = {} # keep track of performance metric for each pole length \n",
    "        self.difficulty_scores = {} # difficulty_scores\n",
    "        self.distribution = {} # probability distribution for pole lengths to sample from\n",
    "        \n",
    "        # initialize difficulties and probabilities (initially prob is uniform)\n",
    "        initial_prob = 1.0 / len(all_pole_lengths)\n",
    "        self.i_p = initial_prob\n",
    "        for length in all_pole_lengths:\n",
    "            self.difficulty_scores[length] = 1.0\n",
    "            self.distribution[length] = initial_prob\n",
    "\n",
    "    def update_rewards(self, pole_length, reward):\n",
    "        self.rewards[pole_length].append(reward)\n",
    "\n",
    "    def update_performances(self, pole_length):\n",
    "        \"\"\"\n",
    "        Here we update the performance of a pole, \n",
    "        \"\"\"\n",
    "        reward_list = self.rewards[pole_length]\n",
    "\n",
    "        # if a pole has not been played yet, it will be assigned a performance metric of 0\n",
    "        if not reward_list:\n",
    "            metric = 0\n",
    "        else:\n",
    "            # here we use a LOOK BACK WINDOW so that we dont use over-stabilized episode rewards\n",
    "            metric = np.mean(reward_list[-self.LOOK_BACK_WINDOW:])\n",
    "        self.performances[pole_length] = metric\n",
    "\n",
    "    def update_difficulties(self):\n",
    "        \"\"\"\n",
    "        Difficulties are inversely proportional to the performance metrics.\n",
    "        The worse a pole length has performed, the higher the diff score (diff=1 being the worst performing, diff=0 being best).\n",
    "        These require global updates as the update is relative to the totality of pole lengths.\n",
    "        Also normalizing + scaling the values down. Metrics can offer quite larger values otherwise. \n",
    "        \"\"\"\n",
    "        M_max = self.find_max()\n",
    "        M_min = self.find_min()\n",
    "        diff_M = M_max - M_min\n",
    "\n",
    "        # update all difficulty scores with new information\n",
    "        for pole_length, metric in self.performances.items():\n",
    "            if diff_M == 0:\n",
    "                difficulty = 1\n",
    "            else:\n",
    "                difficulty = 1 - ((metric-M_min) / diff_M) # if best perform, diff is 0 >>> probability assignment will be 0\n",
    "            self.difficulty_scores[pole_length] = difficulty\n",
    "    \n",
    "    def update_distribution(self):\n",
    "        \"\"\" \n",
    "        Distribution is also global, here we require an update proportional to the totality. \n",
    "        Normalizing the probabilities (between 0 and 1)\n",
    "        \"\"\"\n",
    "        if not self.difficulty_scores:\n",
    "            return\n",
    "        total_difficulty = sum(self.difficulty_scores.values())\n",
    "\n",
    "        if total_difficulty > 0:\n",
    "            for pole_length, difficulty in self.difficulty_scores.items():\n",
    "                self.distribution[pole_length] = difficulty / total_difficulty\n",
    "        else:\n",
    "            for pole_length in self.all_pole_lengths:\n",
    "                self.distribution[pole_length] = self.i_p\n",
    "    \n",
    "    def sample_length(self):\n",
    "        \"\"\"\n",
    "        Here select the pole length using either uniform or categorical prob distribution. \n",
    "        \"\"\"\n",
    "        if random.random() < self.P_ADAPTIVE:\n",
    "            pole_lengths = list(self.distribution.keys())\n",
    "            probs = list(self.distribution.values())\n",
    "\n",
    "            # if our prob dist is empty, we fallback to uniform selection\n",
    "            if not pole_lengths or sum(probs) == 0:\n",
    "                return np.random.choice(self.all_pole_lengths)\n",
    "            \n",
    "            # selection based on categorical sampling (discrete prob distribution)\n",
    "            return np.random.choice(a=pole_lengths, p=probs, size=1)[0]\n",
    "        else:\n",
    "            return np.random.choice(self.all_pole_lengths)\n",
    "\n",
    "    def find_max(self):\n",
    "        \"\"\"\n",
    "        Find max performing pole length\n",
    "        \"\"\"\n",
    "        if not self.performances:\n",
    "            return 0\n",
    "        return max(self.performances.values())\n",
    "\n",
    "    def find_min(self):\n",
    "        \"\"\"\n",
    "        Find min performing pole length\n",
    "        \"\"\"\n",
    "        if not self.performances:\n",
    "            return 0\n",
    "        return min(self.performances.values())\n",
    "    \n",
    "    def calculate_pole_stats(self):\n",
    "        \"\"\"\n",
    "        Nice display function for avg reward of each pole. Not functionally important at all. \n",
    "        \"\"\"\n",
    "        avg_pole_stats = {}\n",
    "        \n",
    "        for pole_length, rewards_list in self.rewards.items():\n",
    "            # Only process if the list is not empty\n",
    "            if rewards_list:\n",
    "                avg_reward = np.mean(rewards_list)\n",
    "                count = len(rewards_list)\n",
    "            else:\n",
    "                avg_reward = 0.0\n",
    "                count = 0\n",
    "            avg_pole_stats[pole_length] = {\n",
    "                \"average_reward\": avg_reward,\n",
    "                \"episode_count\": count\n",
    "            }\n",
    "        for p, avg in avg_pole_stats.items():\n",
    "            print(f\"Pole length {p} has avg reward {avg}\")\n",
    "        return avg_pole_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e184fb50",
   "metadata": {},
   "source": [
    "`3) OUR DEEP Q LEARNING FUNCTIONS: THE TRAINING LOOP AND ACTION PICKER`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f02fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, policy_net, epsilon, action_dim):\n",
    "    \"\"\"\n",
    "    Select action using epsilon-greedy policy - did it with epsilon-greedy because of Assignent 1\n",
    "    \"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(action_dim)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_values = policy_net(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "\n",
    "def deep_q_learning(epsilon, gamma, alpha, q_network, n_episodes,\n",
    "                    uniform_episode_training_cap=None,\n",
    "                    pole_lengths=None, env_name='CartPole-v1',\n",
    "                    batch_size=64, buffer_capacity=50000, \n",
    "                    update_target_every=10, epsilon_decay=0.995, \n",
    "                    epsilon_min=0.01):\n",
    "    \"\"\"\n",
    "    Deep q learning agent for CartPole-v1 environment with varying pole lengths.\n",
    "    \n",
    "    param: epsilon : float - initial exploration rate\n",
    "    param: gamma : float - discount factor\n",
    "    param: alpha : float - learning rate\n",
    "    param: q_network : QNetwork or None - pre-initialized network or None to create new one\n",
    "    param: n_episodes : int - number of training episodes\n",
    "    param: uniform_episode_training_cap : Union[int, None] - number episodes trained with uniform length selection\n",
    "    if None, no adaptive curriculum learning enabled\n",
    "    param: pole_lengths : array-like or None - array of pole lengths to train on (default: linspace(0.4, 1.8, 30))\n",
    "    param: env_name : str - gym environment name\n",
    "    param: batch_size : int - batch size for training\n",
    "    param: buffer_capacity : int - replay buffer capacity\n",
    "    param: update_target_every : int - how often to update target network\n",
    "    param: epsilon_decay : float - epsilon decay rate per episode\n",
    "    param: epsilon_min : float - minimum epsilon value\n",
    "\n",
    "    return: tuple : (policy_net, target_net, episode_returns)\n",
    "        - trained networks and list of episode rewards\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialization of environment\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    # initialization of networks if not provided\n",
    "    if q_network is None:\n",
    "        policy_net = QNetwork(state_dim, action_dim)\n",
    "        target_net = QNetwork(state_dim, action_dim)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        target_net.eval()\n",
    "    else:\n",
    "        policy_net = q_network\n",
    "        target_net = QNetwork(state_dim, action_dim)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        target_net.eval()\n",
    "\n",
    "    # initialization of optimizer\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=alpha)\n",
    "\n",
    "    # initialization of replay buffer\n",
    "    replay_buffer = StratifiedReplayBuffer(buffer_capacity)\n",
    "    \n",
    "    # pole lengths for training\n",
    "    if pole_lengths is None:\n",
    "        pole_lengths = np.linspace(0.4, 1.8, 30)\n",
    "\n",
    "    # storing episode returns for plotting\n",
    "    episode_returns = []\n",
    "\n",
    "    # initialize the acl class for adaptive hyperparametre sampling\n",
    "    if uniform_episode_training_cap is not None:\n",
    "        acl = AdaptiveCurriculumLearning(pole_lengths)\n",
    "    else:\n",
    "        acl = None\n",
    "\n",
    "    # copy of current epsilon value for decay\n",
    "    epsi = epsilon\n",
    "    \n",
    "    # training loop\n",
    "    for episode in range(n_episodes):\n",
    "        # if current episode is below the uniform_episode_training_cap we select from a uniform distr\n",
    "        if uniform_episode_training_cap is None:\n",
    "            pole_length = np.random.choice(pole_lengths)\n",
    "        elif episode <= uniform_episode_training_cap: # or if acl not enabled\n",
    "            pole_length = np.random.choice(pole_lengths)\n",
    "        # else use adaptive probability distrubition \n",
    "        else: \n",
    "            pole_length = acl.sample_length()\n",
    "\n",
    "        env.unwrapped.length = pole_length\n",
    "        \n",
    "        # reset environment\n",
    "        state = env.reset()[0]\n",
    "        episode_reward = 0.0\n",
    "        \n",
    "        # epsilon decay\n",
    "        if epsi > epsilon_min:\n",
    "            epsi = max(epsilon_min, epsi * epsilon_decay)\n",
    "        \n",
    "        # episode loop (1 episode = 1 pole length)\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # select action\n",
    "            action = select_action(state, policy_net, epsi, action_dim)\n",
    "            \n",
    "            # take step\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # store transition in replay buffer\n",
    "            replay_buffer.push(pole_length, state, action, reward, next_state, float(done))\n",
    "\n",
    "            # deep q learning update (using mini-batch from replay buffer)\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                # sample batch from replay buffer\n",
    "                batch = replay_buffer.sample(batch_size)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "                \n",
    "                # convert to tensors \n",
    "                states_t = torch.FloatTensor(states)\n",
    "                actions_t = torch.LongTensor(actions).unsqueeze(1)\n",
    "                rewards_t = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "                next_states_t = torch.FloatTensor(next_states)\n",
    "                dones_t = torch.FloatTensor(dones).unsqueeze(1)\n",
    "\n",
    "                #get current q values\n",
    "                current_q = policy_net(states_t).gather(1, actions_t)\n",
    "                \n",
    "                # target values\n",
    "                with torch.no_grad():\n",
    "                    next_max = target_net(next_states_t).max(1)[0].unsqueeze(1)\n",
    "                    td_target = rewards_t + gamma * next_max * (1 - dones_t)\n",
    "                \n",
    "                # loss calc\n",
    "                loss = nn.MSELoss()(current_q, td_target)\n",
    "                \n",
    "                # backprop and optimize + gradient clipping\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        \n",
    "        \n",
    "        #update target network periodically\n",
    "        if episode % update_target_every == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        #store episode reward\n",
    "        episode_returns.append(episode_reward)\n",
    "        \n",
    "        # update acl for rewards, performance metrics, scores, and probs\n",
    "        # difficulty scores and probabilities are global updates, can be seen in the respective update methods\n",
    "        if uniform_episode_training_cap is not None:\n",
    "            acl.update_rewards(pole_length, episode_reward)\n",
    "            if episode >= uniform_episode_training_cap:\n",
    "                acl.update_performances(pole_length)\n",
    "                acl.update_difficulties()\n",
    "                acl.update_distribution()\n",
    "\n",
    "        #only for seeing the progress\n",
    "        if episode % 100 == 0:\n",
    "            avg_reward = np.mean(episode_returns[-100:]) if len(episode_returns) >= 100 else np.mean(episode_returns)\n",
    "            print(f\"Episode {episode}/{n_episodes} | \"\n",
    "                  f\"Avg Reward: {avg_reward:.1f} | \"\n",
    "                  f\"Epsilon: {epsi:.3f} | \"\n",
    "                  )            \n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    return policy_net, target_net, episode_returns, acl, uniform_episode_training_cap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534a983c",
   "metadata": {},
   "source": [
    "`4) TRAINING, AND SAVING THE MODEL`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c3d3eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/2000 | Avg Reward: 9.0 | Epsilon: 0.980 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hleca\\AppData\\Local\\Temp\\ipykernel_7020\\4071974201.py:124: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:256.)\n",
      "  states_t = torch.FloatTensor(states)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100/2000 | Avg Reward: 114.5 | Epsilon: 0.130 | \n",
      "Episode 200/2000 | Avg Reward: 255.1 | Epsilon: 0.050 | \n",
      "Episode 300/2000 | Avg Reward: 132.8 | Epsilon: 0.050 | \n",
      "Episode 400/2000 | Avg Reward: 178.2 | Epsilon: 0.050 | \n",
      "Episode 500/2000 | Avg Reward: 366.7 | Epsilon: 0.050 | \n",
      "Episode 600/2000 | Avg Reward: 413.8 | Epsilon: 0.050 | \n",
      "Episode 700/2000 | Avg Reward: 321.6 | Epsilon: 0.050 | \n",
      "Episode 800/2000 | Avg Reward: 351.4 | Epsilon: 0.050 | \n",
      "Episode 900/2000 | Avg Reward: 306.6 | Epsilon: 0.050 | \n",
      "Episode 1000/2000 | Avg Reward: 277.2 | Epsilon: 0.050 | \n",
      "Episode 1100/2000 | Avg Reward: 407.3 | Epsilon: 0.050 | \n",
      "Episode 1200/2000 | Avg Reward: 232.1 | Epsilon: 0.050 | \n",
      "Episode 1300/2000 | Avg Reward: 270.6 | Epsilon: 0.050 | \n",
      "Episode 1400/2000 | Avg Reward: 262.0 | Epsilon: 0.050 | \n",
      "Episode 1500/2000 | Avg Reward: 298.3 | Epsilon: 0.050 | \n",
      "Episode 1600/2000 | Avg Reward: 221.8 | Epsilon: 0.050 | \n",
      "Episode 1700/2000 | Avg Reward: 329.6 | Epsilon: 0.050 | \n",
      "Episode 1800/2000 | Avg Reward: 329.7 | Epsilon: 0.050 | \n",
      "Episode 1900/2000 | Avg Reward: 273.6 | Epsilon: 0.050 | \n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'acl' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m torch.manual_seed(seed)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# --- training test ---\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m policy_net, target_net, episode_returns, acl, training_cap = \u001b[43mdeep_q_learning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mq_network\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# create fresh nets\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43muniform_episode_training_cap\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# here you set the moment from which adaptive CL is applied \u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpole_lengths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinspace\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mCartPole-v1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbuffer_capacity\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mupdate_target_every\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepsilon_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.98\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepsilon_min\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.05\u001b[39;49m\n\u001b[32m     22\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m----finished training----\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m policy_net.save(\u001b[33m'\u001b[39m\u001b[33mtrained_models_rb_strategy\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 177\u001b[39m, in \u001b[36mdeep_q_learning\u001b[39m\u001b[34m(epsilon, gamma, alpha, q_network, n_episodes, uniform_episode_training_cap, pole_lengths, env_name, batch_size, buffer_capacity, update_target_every, epsilon_decay, epsilon_min)\u001b[39m\n\u001b[32m    170\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_episodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    171\u001b[39m               \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAvg Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    172\u001b[39m               \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpsilon: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepsi\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    173\u001b[39m               )            \n\u001b[32m    175\u001b[39m env.close()\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m policy_net, target_net, \u001b[43mepisode_returns\u001b[49m, acl, uniform_episode_training_cap\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'acl' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "# --- IGNORE ---\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# --- training test ---\n",
    "policy_net, target_net, episode_returns, acl, training_cap = deep_q_learning(\n",
    "    epsilon=1.0,\n",
    "    gamma=0.99,\n",
    "    alpha=1e-3,\n",
    "    q_network=None,           # create fresh nets\n",
    "    n_episodes=2000, \n",
    "    uniform_episode_training_cap=None, # here you set the moment from which adaptive CL is applied \n",
    "    pole_lengths=np.linspace(0.4, 1.2, 30),\n",
    "    env_name='CartPole-v1',\n",
    "    batch_size=32,\n",
    "    buffer_capacity=10000,\n",
    "    update_target_every=5,\n",
    "    epsilon_decay=0.98,\n",
    "    epsilon_min=0.05\n",
    ")\n",
    "\n",
    "print(\"----finished training----\")\n",
    "\n",
    "policy_net.save('trained_models_rb_strategy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea7a3b8",
   "metadata": {},
   "source": [
    "`5) OUR PLOTTING FUNCTION`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ce40d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_episode_rewards_averaged(episode_rewards, episode_cap=None, window_size=50):\n",
    "    rewards_series = np.array(episode_rewards)\n",
    "    \n",
    "    weights = np.ones(window_size) / window_size\n",
    "    averaged_rewards = np.convolve(rewards_series, weights, mode='valid')\n",
    "\n",
    "    averaged_episodes = np.arange(window_size, len(episode_rewards) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.plot(averaged_episodes, averaged_rewards, \n",
    "             label=f'Rolling Average (Window = {window_size})', \n",
    "             color='b') \n",
    "    \n",
    "    # check we are using acl training\n",
    "    if episode_cap > 0 and episode_cap <= len(episode_rewards) and episode_cap is not None:\n",
    "        plt.axvline(x=episode_cap, color='r', linestyle='--', \n",
    "                    label=f'Training Shift (Episode {episode_cap})')\n",
    "    \n",
    "    raw_episodes = range(1, len(episode_rewards) + 1)\n",
    "    plt.plot(raw_episodes, episode_rewards, \n",
    "             label='Raw Episode Reward', \n",
    "             color='gray', \n",
    "             alpha=0.3)\n",
    "    \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel(f'Reward (Avg over {window_size} Episodes)')\n",
    "    plt.title(f'Episode Reward Rolling Average (Window {window_size})')\n",
    "    \n",
    "    plt.xlim(0, len(episode_rewards) + 1)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.savefig('averaged_episode_rewards_plot.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27b93f8",
   "metadata": {},
   "source": [
    "`6) PLOTTING THE RESULTS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014e1bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_episode_rewards_averaged(episode_returns, training_cap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "introrl25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
